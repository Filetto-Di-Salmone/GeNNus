{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_from_str_to_one_hot(label_str: str): \n",
    "  \n",
    "  if label_str == \"Pop\":\n",
    "    return torch.tensor([1, 0, 0, 0, 0, 0])\n",
    "  \n",
    "  if label_str == \"Hip-Hop\":\n",
    "    return torch.tensor([0, 1, 0, 0, 0, 0])\n",
    "  \n",
    "  if label_str == \"Electronic\":\n",
    "    return torch.tensor([0, 0, 1, 0, 0, 0])\n",
    "  \n",
    "  if label_str == \"Rock\":\n",
    "    return torch.tensor([0, 0, 0, 1, 0, 0])\n",
    "\n",
    "  if label_str == \"Folk\":\n",
    "    return torch.tensor([0, 0, 0, 0, 1, 0])\n",
    "\n",
    "  if label_str == \"Jazz\":\n",
    "    return torch.tensor([0, 0, 0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_audio_data(\n",
    "  path, normalize_audio, audio_num_frames\n",
    "):\n",
    "  \n",
    "  audio_tensor_list = []\n",
    "\n",
    "  for path, subdirs, files in tqdm(os.walk(path), colour=\"teal\"):\n",
    "    for name in tqdm(files, colour=\"turquoise\"):\n",
    "        \n",
    "      file_audio_path = os.path.join(path, name)\n",
    "      \n",
    "      try:\n",
    "        waveform, sample_rate = torchaudio.load(\n",
    "          file_audio_path, normalize=normalize_audio,\n",
    "          num_frames=audio_num_frames\n",
    "        )\n",
    "        \n",
    "        label = file_audio_path.split(\"/\")[-2]\n",
    "        label_one_hot = label_from_str_to_one_hot(label)\n",
    "        \n",
    "        audio_tensor_list.append(\n",
    "          {\n",
    "            \"waveform\": waveform, \n",
    "            \"sample_rate\": sample_rate,\n",
    "            \"label_one_hot\": label_one_hot,\n",
    "            \"label\": label\n",
    "          }\n",
    "        )\n",
    "        \n",
    "      except:\n",
    "        print(f\"[load_audio_data] error while loading {file_audio_path}\")\n",
    "        continue\n",
    "  \n",
    "  return audio_tensor_list\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"audio_hop\" --> take one sample ever hop_size elements\n",
    "def perform_audio_hop(og_audio_list, hop_size):\n",
    "  \n",
    "  resampled_audio_tensor_list = []\n",
    "  \n",
    "  for audio in tqdm(og_audio_list, colour=\"steelblue\"):\n",
    "    \n",
    "    resampled_waveform = torchaudio.functional.resample(\n",
    "        audio[\"waveform\"], orig_freq=hop_size, new_freq=1\n",
    "      )\n",
    "    \n",
    "    resampled_audio_tensor_list.append(\n",
    "      {\n",
    "        \"waveform\": resampled_waveform.numpy(), \n",
    "        \"og_sample_rate\": audio[\"sample_rate\"],\n",
    "        \"hop_size\": hop_size,\n",
    "        \"label_one_hot\": audio[\"label_one_hot\"].numpy(),\n",
    "        \"label\": audio[\"label\"]\n",
    "      }\n",
    "    )\n",
    "    \n",
    "  return resampled_audio_tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2890a45c10f04b01adb611260b91c7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8f38ec1a0248d3a659b1fa8adaecd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a91b0201a8c463eba0952a3f410a33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3705bcf87ef407faef681106a6a14ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a916c2ac3c3d4ac69ea9f34f868462f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fbdef117ee49a6a45083c225cf7931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794003655b8e4e5280700cf8620994dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b6bf3044de43b8954c1b582d48763b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "og_audio_list = load_raw_audio_data(\n",
    "  path=\"./data/fma_extra_small_organized_by_label/\", \n",
    "  normalize_audio=True, \n",
    "  audio_num_frames=1320000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463f551aaa0643848c70c70cd749988c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hop_size = 512\n",
    "\n",
    "hopped_audio_list = perform_audio_hop(og_audio_list, hop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from: https://stackoverflow.com/a/47626762\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "  \n",
    "  def default(self, obj):\n",
    "    \n",
    "    if isinstance(obj, np.ndarray):\n",
    "      return obj.tolist()\n",
    "    \n",
    "    return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def export_pre_processed_data(\n",
    "  pre_processed_data, export_path: str, export_name: str\n",
    "):\n",
    "  \n",
    "  with open(f\"{export_path}/{export_name}\", \"w\") as final:\n",
    "    json.dump(pre_processed_data, final, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_pre_processed_data(hopped_audio_list, \"./data\", \"fma_extra_small_hop_512.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD CODE TO RESAMPLE AND GET A MEL SPECTROGRAM\n",
    "\n",
    "# waveform = torchaudio.functional.resample(\n",
    "#           waveform, orig_freq=audio_hop_length, new_freq=1\n",
    "#         )\n",
    "        \n",
    "\n",
    "        \n",
    "#         temp_dict_waveform = {\n",
    "#           \"label\": label_one_hot,\n",
    "#           \"waveform\": waveform\n",
    "#         }\n",
    "        \n",
    "#         temp_dict_mel_spectrogram = {\n",
    "#           \"label\": label_one_hot,\n",
    "#           \"mel_spectrogram\": self.get_mel_spectrogram(\n",
    "#             waveform, sample_rate\n",
    "#           )\n",
    "#         }\n",
    "        \n",
    "#         data_waveform.append(temp_dict_waveform)\n",
    "#         data_mel_spectrogram.append(temp_dict_mel_spectrogram)\n",
    "        \n",
    "#   return data_waveform, data_mel_spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConverter(Dataset):\n",
    "    def __init__(\n",
    "      self, path: str, use_spectrogram: bool, normalize_audio: bool, \n",
    "      audio_num_frames: int, audio_hop_length: int\n",
    "    ):\n",
    "      self.path = path\n",
    "      self.use_spectrogram = use_spectrogram\n",
    "      self.normalize_audio = normalize_audio\n",
    "      self.audio_num_frames = audio_num_frames\n",
    "      self.audio_hop_length = audio_hop_length\n",
    "      \n",
    "      # TODO load the raw dataset only once, then use it in another class/cell\n",
    "      # to perform all the desired \"compressions\"/samplings/whatever \n",
    "      self.data_waveform, self.data_mel_spectrogram = self.load_audio_data()\n",
    "      \n",
    "      # TODO export self.data_waveform to disk\n",
    "      # TODO export self.data_spectrogram to disk\n",
    "  \n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      return self.data_waveform[idx], self.data_mel_spectrogram[idx]\n",
    "    \n",
    "\n",
    "      \n",
    "    def get_mel_spectrogram(self, waveform, sample_rate):\n",
    "      \n",
    "      # TODO save the MelSpectrogram object in the class constructor in order\n",
    "      # to avoid to re-init it every time\n",
    "      \n",
    "      n_fft = 1024\n",
    "      win_length = None\n",
    "      # hop_length = 512\n",
    "      hop_length = 1\n",
    "      n_mels = 128\n",
    "\n",
    "      mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm='slaney',\n",
    "        onesided=True,\n",
    "        n_mels=n_mels,\n",
    "        mel_scale=\"htk\",\n",
    "      )\n",
    "\n",
    "      return mel_spectrogram_transform(waveform)\n",
    "    \n",
    "    def load_audio_data(self):\n",
    "      \n",
    "      audio_file_list = []\n",
    "      data_waveform, data_mel_spectrogram = [], []\n",
    "\n",
    "      for path, subdirs, files in tqdm(os.walk(self.path), colour=\"teal\"):\n",
    "        for name in tqdm(files, colour=\"turquoise\"):\n",
    "            \n",
    "            file_audio_path = os.path.join(path, name)\n",
    "            \n",
    "            try:\n",
    "              waveform, sample_rate = torchaudio.load(\n",
    "                file_audio_path, normalize=self.normalize_audio,\n",
    "                num_frames=self.audio_num_frames\n",
    "              )\n",
    "            except:\n",
    "              print(f\"Got an error while loading {file_audio_path}\")\n",
    "              continue\n",
    "            \n",
    "            waveform = torchaudio.functional.resample(\n",
    "              waveform, orig_freq=self.audio_hop_length, new_freq=1\n",
    "              # orig_freq=sample_rate, \n",
    "              # new_freq=sample_rate / self.audio_hop_length\n",
    "            )\n",
    "            \n",
    "            label = file_audio_path.split(\"/\")[-2]\n",
    "            label_one_hot = self.label_from_str_to_one_hot(label)\n",
    "            \n",
    "            temp_dict_waveform = {\n",
    "              \"label\": label_one_hot,\n",
    "              \"waveform\": waveform\n",
    "            }\n",
    "            \n",
    "            temp_dict_mel_spectrogram = {\n",
    "              \"label\": label_one_hot,\n",
    "              \"mel_spectrogram\": self.get_mel_spectrogram(\n",
    "                waveform, sample_rate\n",
    "              )\n",
    "            }\n",
    "            \n",
    "            data_waveform.append(temp_dict_waveform)\n",
    "            data_mel_spectrogram.append(temp_dict_mel_spectrogram)\n",
    "            \n",
    "      return data_waveform, data_mel_spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_converter = DatasetConverter(\n",
    "  path=\"./data/fma_extra_small_organized_by_label/\", \n",
    "  # path=\"./data/fma_large_6_top_level_downsampled_organized_by_label/\", \n",
    "  # KEEP THIS SET TO TRUE WHILE EXPORTING THE DATASETS IN THE \"COMPRESSED\" FORMAT\n",
    "  use_spectrogram=True, \n",
    "  normalize_audio=True, audio_num_frames=1320000, \n",
    "  audio_hop_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_converter.data_mel_spectrogram[3][\"mel_spectrogram\"].shape)\n",
    "print(dataset_converter.data_waveform[3][\"waveform\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
