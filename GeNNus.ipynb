{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import gc\n",
    "\n",
    "MANUAL_SEED = 69\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMADataset(Dataset):\n",
    "\n",
    "  def __init__(self, path, normalize_audio, audio_num_frames):\n",
    "    self.path = path\n",
    "    self.normalize_audio = normalize_audio\n",
    "    self.audio_num_frames = audio_num_frames\n",
    "    \n",
    "    self.data = self._load_audio_list()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \n",
    "    try: \n",
    "      \n",
    "      waveform, _ = torchaudio.load(\n",
    "        filepath=self.data[idx], \n",
    "        normalize=self.normalize_audio,\n",
    "        num_frames=self.audio_num_frames\n",
    "      )\n",
    "      \n",
    "      if waveform.shape[1] < self.audio_num_frames:\n",
    "        waveform = self._apply_padding(waveform)\n",
    "      \n",
    "      label = self.data[idx].split(\"/\")[-2]\n",
    "      label_one_hot = self._label_from_str_to_one_hot(label)\n",
    "      \n",
    "      return waveform, label_one_hot\n",
    "    \n",
    "    except Exception as e:\n",
    "      print(f\"Got the following exception for the file {self.data[idx]}\")\n",
    "      print(\"\\n\\n\")\n",
    "      print(e)\n",
    "      \n",
    "  def _apply_padding(self, to_pad):\n",
    "    padding_size = self.audio_num_frames - to_pad.shape[1]\n",
    "    \n",
    "    return torch.nn.functional.pad(\n",
    "      to_pad, (0, padding_size)\n",
    "    )\n",
    "  \n",
    "  def _label_from_str_to_one_hot(self, label_str: str): \n",
    "  \n",
    "    if label_str == \"Pop\":\n",
    "      return torch.tensor([1, 0, 0, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Hip-Hop\":\n",
    "      return torch.tensor([0, 1, 0, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Electronic\":\n",
    "      return torch.tensor([0, 0, 1, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Rock\":\n",
    "      return torch.tensor([0, 0, 0, 1, 0, 0]).float()\n",
    "\n",
    "    if label_str == \"Folk\":\n",
    "      return torch.tensor([0, 0, 0, 0, 1, 0]).float()\n",
    "\n",
    "    if label_str == \"Jazz\":\n",
    "      return torch.tensor([0, 0, 0, 0, 0, 1]).float()\n",
    "    \n",
    "  \n",
    "  def _load_audio_list(self):\n",
    "    \n",
    "    audio_path_list = []\n",
    "    \n",
    "    for path, subdirs, files in tqdm(os.walk(self.path), colour=\"magenta\"):\n",
    "      for name in files:\n",
    "          \n",
    "        file_audio_path = os.path.join(path, name)\n",
    "        \n",
    "        audio_path_list.append(file_audio_path)\n",
    "        \n",
    "    return audio_path_list\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = \"extra_small\"\n",
    "DATASET_NAME = f\"fma_{DATASET_SIZE}_organized_by_label_resampled_rechanneled\"\n",
    "# DATASET_NAME = f\"fma_{DATASET_SIZE}_organized_by_label\"\n",
    "DATASET_FOLDER = \"./data/audio\"\n",
    "# DATASET_FOLDER = \"/mnt/ramdisk\"\n",
    "\n",
    "dataset_path = f\"{DATASET_FOLDER}/{DATASET_NAME}\"\n",
    "\n",
    "NORMALIZE_AUDIO = True\n",
    "AUDIO_NUM_FRAMES = 238000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542670bc017f4af4bba6d600c08667de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fma_dataset = FMADataset(\n",
    "fma_dataset = FMADataset(\n",
    "  path=dataset_path, \n",
    "  normalize_audio=NORMALIZE_AUDIO, \n",
    "  audio_num_frames=AUDIO_NUM_FRAMES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(fma_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERCENTAGE = 0.7\n",
    "VAL_PERCENTAGE = 0.2\n",
    "# TEST_PERCENTAGE = 0.1\n",
    "\n",
    "full_size = len(fma_dataset)\n",
    "train_size = int(TRAIN_PERCENTAGE * len(fma_dataset))\n",
    "val_size = int(VAL_PERCENTAGE * len(fma_dataset))\n",
    "test_size = full_size - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=torch.Generator().manual_seed(MANUAL_SEED)\n",
    "\n",
    "fma_dataset_train, fma_dataset_val, fma_dataset_test = torch.utils.data.random_split(\n",
    "  fma_dataset, [train_size, val_size, test_size], generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"len(fma_dataset_train): {len(fma_dataset_train)}\")\n",
    "# print(f\"len(fma_dataset_val)  : {len(fma_dataset_val)}\")\n",
    "# print(f\"len(fma_dataset_test) : {len(fma_dataset_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_batch_size_compatibility(batch_size, dataloader_len):\n",
    "  return dataloader_len % batch_size == 1\n",
    "\n",
    "def show_batch_sizes_warnings(\n",
    "  batch_size, train_dl, val_dl, test_dl\n",
    "):\n",
    "\n",
    "  batch_size_warning_train = check_batch_size_compatibility(\n",
    "    batch_size=batch_size, dataloader_len=len(train_dl)\n",
    "  )\n",
    "  \n",
    "  if batch_size_warning_train:\n",
    "    print(\n",
    "      f\"WARNING: batch_size {batch_size} may give size error while using \" + \n",
    "      f\"batch_norm with dataloader of len {len(train_dl)}\"\n",
    "    )\n",
    "  \n",
    "  batch_size_warning_val = check_batch_size_compatibility(\n",
    "    batch_size=batch_size, dataloader_len=len(val_dl)\n",
    "  )\n",
    "  \n",
    "  if batch_size_warning_val:\n",
    "    print(\n",
    "      f\"WARNING: batch_size {batch_size} may give size error while using \" + \n",
    "      f\"batch_norm with dataloader of len {len(val_dl)}\"\n",
    "    )\n",
    "  \n",
    "  batch_size_warning_test = check_batch_size_compatibility(\n",
    "    batch_size=batch_size, dataloader_len=len(test_dl)\n",
    "  )\n",
    "  \n",
    "  if batch_size_warning_test:\n",
    "    print(\n",
    "      f\"WARNING: batch_size {batch_size} may give size error while using \" + \n",
    "      f\"batch_norm with dataloader of len {len(test_dl)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "NUM_WORKERS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fma_dataloader_train = torch.utils.data.DataLoader(\n",
    "  fma_dataset_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, \n",
    "  generator=generator\n",
    ")\n",
    "fma_dataloader_val = torch.utils.data.DataLoader(\n",
    "  fma_dataset_val, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, \n",
    "  generator=generator\n",
    ")\n",
    "fma_dataloader_test = torch.utils.data.DataLoader(\n",
    "  fma_dataset_test, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, \n",
    "  generator=generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_output_length(length_in, kernel_size, stride=1, padding=0, dilation=1):\n",
    "  return (\n",
    "    length_in + 2 * padding - dilation * (kernel_size - 1) - 1\n",
    "  ) // stride + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_num_trainable_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1\n",
    "\n",
    "First layers --> dimensionality reduction to roughly match dimensions of this paper https://arxiv.org/pdf/1703.01789.pdf\n",
    "\n",
    "Mid and final layers --> taken 1:1 from the paper linked above\n",
    "\n",
    "Batch norm placed BEFORE the activation function, as described in the og paper https://arxiv.org/abs/1502.03167 and explained by Bengio in his DL book https://www.deeplearningbook.org/contents/optimization.html in section 8.7.1\n",
    "\n",
    "Dropout placed according to the og paper: https://arxiv.org/pdf/1207.0580.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Attempt_1(nn.Module):\n",
    "  def __init__(self, dropout_p):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.dropout_p = dropout_p\n",
    "    \n",
    "    # First layers, see cell above for full explanation \n",
    "    \n",
    "    self.conv1 = nn.Conv1d(\n",
    "      in_channels=1, out_channels=128, kernel_size=6, stride=4\n",
    "    )\n",
    "    \n",
    "    # Mid and final layers, see cell above for full explanation\n",
    "    \n",
    "    self.conv3 = nn.Conv1d(\n",
    "      in_channels=128, out_channels=128, kernel_size=3, stride=3\n",
    "    ) \n",
    "    \n",
    "    self.conv4 = nn.Conv1d(\n",
    "      in_channels=128, out_channels=128, kernel_size=3, stride=1\n",
    "    )\n",
    "    self.pool4 = nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "    \n",
    "    self.conv5 = nn.Conv1d(\n",
    "      in_channels=128, out_channels=128, kernel_size=3, stride=1\n",
    "    )\n",
    "    self.pool5 = nn.MaxPool1d(kernel_size=3, stride=3)    \n",
    "    \n",
    "    self.conv6 = nn.Conv1d(\n",
    "      in_channels=128, out_channels=256, kernel_size=3, stride=1\n",
    "    )\n",
    "    self.pool6 = nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "    \n",
    "    self.conv7 = nn.Conv1d(\n",
    "      in_channels=256, out_channels=256, kernel_size=3, stride=1\n",
    "    )\n",
    "    self.pool7 = nn.MaxPool1d(kernel_size=3, stride=3)      \n",
    "    \n",
    "    self.conv8 = nn.Conv1d(\n",
    "      in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "    self.pool8 = nn.MaxPool1d(kernel_size=3,  stride=3)      \n",
    "    \n",
    "    self.conv9 = nn.Conv1d(\n",
    "      in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "    self.pool9 = nn.MaxPool1d(kernel_size=3,  stride=3)      \n",
    "    \n",
    "    self.conv10 = nn.Conv1d(\n",
    "      in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "    self.pool10 = nn.MaxPool1d(kernel_size=3,  stride=3)      \n",
    "    \n",
    "    self.conv11 = nn.Conv1d(\n",
    "      in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "    self.pool11 = nn.MaxPool1d(kernel_size=3,  stride=3)      \n",
    "\n",
    "    self.conv12 = nn.Conv1d(\n",
    "      in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "    self.pool12 = nn.MaxPool1d(kernel_size=3,  stride=3)      \n",
    "\n",
    "    self.conv13 = nn.Conv1d(\n",
    "      in_channels=512, out_channels=512, kernel_size=1, stride=1\n",
    "    )\n",
    "    \n",
    "    # Classification layer\n",
    "    \n",
    "    # Using a conv output layer rather than a fully connected one\n",
    "    self.conv14 = nn.Conv1d(\n",
    "      in_channels=512, out_channels=6, kernel_size=1\n",
    "    )\n",
    "    \n",
    "    self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "    \n",
    "    self.relu = nn.ReLU()\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    self.bn_16 = nn.BatchNorm1d(num_features=16)       \n",
    "    self.bn_32 = nn.BatchNorm1d(num_features=32)       \n",
    "    self.bn_128 = nn.BatchNorm1d(num_features=128)       \n",
    "    self.bn_256 = nn.BatchNorm1d(num_features=256)       \n",
    "    self.bn_512 = nn.BatchNorm1d(num_features=512)       \n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    # First layers, see cell above for full explanation \n",
    "    \n",
    "    x = self.conv1(x)\n",
    "    x = self.bn_128(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    # print(\"-2. x.shape\", x.shape)\n",
    "    \n",
    "    # Mid and final layers, see cell above for full explanation \n",
    "    \n",
    "    x = self.conv3(x)\n",
    "    x = self.bn_128(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    # print(\"3. x.shape\", x.shape)\n",
    "\n",
    "    x = self.conv4(x)\n",
    "    x = self.pool4(x)\n",
    "    x = self.bn_128(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    # print(\"4. x.shape\", x.shape)\n",
    "    \n",
    "    x = self.conv5(x)\n",
    "    x = self.pool5(x)\n",
    "    x = self.bn_128(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    # print(\"5. x.shape\", x.shape)\n",
    "\n",
    "    x = self.conv6(x)\n",
    "    x = self.pool6(x)\n",
    "    x = self.bn_256(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    # print(\"6. x.shape\", x.shape)\n",
    "    \n",
    "    x = self.conv7(x)\n",
    "    x = self.pool7(x)\n",
    "    x = self.bn_256(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    # print(\"7. x.shape\", x.shape)\n",
    "    \n",
    "    x = self.conv8(x)\n",
    "    # print(\"8_conv. x.shape\", x.shape)\n",
    "    x = self.pool8(x)\n",
    "    # print(\"7_pool. x.shape\", x.shape)\n",
    "    x = self.bn_256(x)\n",
    "    x = self.relu(x)\n",
    "  \n",
    "    x = self.conv9(x)\n",
    "    # print(\"9_conv. x.shape\", x.shape)\n",
    "    x = self.pool9(x)\n",
    "    # print(\"8_pool. x.shape\", x.shape)\n",
    "    x = self.bn_256(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    \n",
    "    x = self.conv10(x)\n",
    "    # print(\"10_conv. x.shape: \", x.shape)\n",
    "    x = self.pool10(x)\n",
    "    # print(\"9_pool. x.shape: \", x.shape)\n",
    "    x = self.bn_256(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    x = self.conv11(x)\n",
    "    # print(\"11_conv. x.shape: \", x.shape)\n",
    "    x = self.pool11(x)\n",
    "    # print(\"11_pool. x.shape: \", x.shape)\n",
    "    x = self.bn_256(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    x = self.conv12(x)\n",
    "    # print(\"12_conv. x.shape: \", x.shape)\n",
    "    x = self.pool12(x)\n",
    "    # print(\"11_pool. x.shape: \", x.shape)\n",
    "    x = self.bn_512(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    x = self.conv13(x)\n",
    "    # print(\"13_conv. x.shape: \", x.shape)\n",
    "    x = self.bn_512(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.dropout(x)\n",
    "    \n",
    "    # Classification layer\n",
    "    x = self.conv14(x)\n",
    "    x = self.sigmoid(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_attempt_1 = CNN_Attempt_1(dropout_p=0.5)\n",
    "\n",
    "x = torch.rand((16, 1, 238000))\n",
    "\n",
    "x_out = cnn_attempt_1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1892966"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_num_trainable_parameters(cnn_attempt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "  cnn_attempt_1.parameters(), \n",
    "  lr=LR, \n",
    "  momentum=MOMENTUM,\n",
    "  nesterov=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_attempt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(\n",
    "  torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct_preds(outputs, labels):\n",
    "  \n",
    "  output_pred_ind = torch.argmax(outputs, dim=1)\n",
    "  labels_ind = torch.argmax(labels, dim=1)\n",
    "  \n",
    "  matching_mask = (output_pred_ind == labels_ind).float()\n",
    "  \n",
    "  num_correct_preds = matching_mask.sum()\n",
    "  \n",
    "  return num_correct_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 6\n",
    "PRINT_FREQ = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "  model, batch_size, train_dl, val_dl, test_dl, num_epochs, device\n",
    "):\n",
    "  \n",
    "  # show_batch_sizes_warnings(batch_size, train_dl, val_dl, test_dl)\n",
    "  \n",
    "  model = model.to(device)\n",
    "  \n",
    "  pbar_epochs = tqdm(range(num_epochs), colour=\"#9400d3\")\n",
    "  \n",
    "  pbar_batches_train = tqdm(\n",
    "    iter(train_dl), colour=\"#4169e1\", leave=False,\n",
    "  )\n",
    "  \n",
    "  pbar_batches_val = tqdm(\n",
    "    iter(val_dl), colour=\"#008080\", leave=False,\n",
    "  )\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss_train = 0.0\n",
    "    running_loss_val   = 0.0\n",
    "    \n",
    "    num_correct_preds_train = 0.0\n",
    "    num_preds_train = 0.0\n",
    "    accuracy_train = 0.0\n",
    "    \n",
    "    num_correct_preds_val = 0.0\n",
    "    num_preds_val = 0.0\n",
    "    accuracy_val = 0.0\n",
    "    \n",
    "    ## BEGIN training step\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    pbar_batches_train.reset()\n",
    "    pbar_batches_val.reset()\n",
    "    pbar_epochs.set_description(f\"epoch {epoch}\")\n",
    "    pbar_batches_train.set_description(f\"epoch {epoch}\")\n",
    "    pbar_batches_val.set_description  (f\"epoch {epoch}\")\n",
    "    \n",
    "    for batch_x, batch_y in iter(train_dl):\n",
    "\n",
    "      inputs, labels = batch_x, batch_y\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      outputs = outputs.squeeze(-1)\n",
    "      \n",
    "      loss = criterion(outputs, labels)\n",
    "      \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # running_loss_train += loss.item() * inputs.shape[0]\n",
    "      running_loss_train += loss.item()\n",
    "      \n",
    "      num_correct_preds_train += get_num_correct_preds(outputs, labels)\n",
    "      num_preds_train += outputs.shape[0]\n",
    "      \n",
    "      pbar_batches_train.update(1)\n",
    "      \n",
    "    \n",
    "    ## END training step\n",
    "    \n",
    "    ## BEGIN validation step\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      \n",
    "      model.eval()\n",
    "      \n",
    "      for batch_x, batch_y in iter(val_dl):\n",
    "\n",
    "        inputs, labels = batch_x, batch_y\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(-1)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # running_loss_val += loss.item() * inputs.shape[0]\n",
    "        running_loss_val += loss.item()\n",
    "        \n",
    "        num_correct_preds_val += get_num_correct_preds(outputs, labels)\n",
    "        num_preds_val += outputs.shape[0]\n",
    "        \n",
    "        pbar_batches_val.update(1)\n",
    "        \n",
    "    ## END validation step\n",
    "    \n",
    "    accuracy_train = num_correct_preds_train / num_preds_train\n",
    "    accuracy_val = num_correct_preds_val / num_preds_val\n",
    "    \n",
    "    if ((epoch + 1) % PRINT_FREQ == 0):  \n",
    "      tqdm.write(\n",
    "        f\"epoch: {epoch + 1}, \" + \n",
    "        f\"train loss: {running_loss_train}, train acc: {accuracy_train}, \" + \n",
    "        f\"val loss: {running_loss_val}, val acc: {accuracy_val}, \"\n",
    "      )\n",
    "    \n",
    "    pbar_epochs.update(1)\n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e752a5e70f48f3bb4c8324ce0bc755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7396d1ce1fd645b99f2cf43fea713464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8421e868b954d1da4bee592753ce12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, train loss: 9.739505171775818, train acc: 0.7142857313156128, val loss: 3.663582444190979, val acc: 0.1666666716337204, \n",
      "epoch: 4, train loss: 9.554660201072693, train acc: 0.761904776096344, val loss: 3.562304377555847, val acc: 0.1666666716337204, \n",
      "epoch: 6, train loss: 9.263745307922363, train acc: 0.6190476417541504, val loss: 3.6099531650543213, val acc: 0.1666666716337204, \n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "  model=model, \n",
    "  batch_size=BATCH_SIZE,\n",
    "  train_dl=fma_dataloader_train, \n",
    "  val_dl=fma_dataloader_val,\n",
    "  test_dl=fma_dataloader_test,\n",
    "  num_epochs=NUM_EPOCHS,\n",
    "  device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
