{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import gc\n",
    "\n",
    "MANUAL_SEED = 69\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "  \n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "import copy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMADataset(Dataset):\n",
    "\n",
    "  def __init__(\n",
    "    self, path, normalize_audio, audio_num_frames, mean, std\n",
    "  ):\n",
    "    self.path = path\n",
    "    self.normalize_audio = normalize_audio\n",
    "    self.audio_num_frames = audio_num_frames\n",
    "    self.mean = mean\n",
    "    self.std = std\n",
    "    \n",
    "    self.data = self._load_audio_list()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \n",
    "    try: \n",
    "      \n",
    "      waveform, _ = torchaudio.load(\n",
    "        filepath=self.data[idx], \n",
    "        normalize=self.normalize_audio,\n",
    "        num_frames=self.audio_num_frames\n",
    "      )\n",
    "      \n",
    "      if waveform.shape[1] < self.audio_num_frames:\n",
    "        waveform = self._apply_padding(waveform)\n",
    "      \n",
    "      label = self.data[idx].split(\"/\")[-2]\n",
    "      label_one_hot = self._label_from_str_to_one_hot(label)\n",
    "\n",
    "      waveform_normalized = (waveform - self.mean) / self.std\n",
    "      \n",
    "      return waveform_normalized, label_one_hot\n",
    "    \n",
    "    except Exception as e:\n",
    "      print(f\"Got the following exception for the file {self.data[idx]}\")\n",
    "      print(\"\\n\\n\")\n",
    "      print(e)\n",
    "      \n",
    "  def _apply_padding(self, to_pad):\n",
    "    padding_size = self.audio_num_frames - to_pad.shape[1]\n",
    "    \n",
    "    return torch.nn.functional.pad(\n",
    "      to_pad, (0, padding_size)\n",
    "    )\n",
    "  \n",
    "  def _label_from_str_to_one_hot(self, label_str: str): \n",
    "  \n",
    "    if label_str == \"Pop\":\n",
    "      return torch.tensor([1, 0, 0, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Hip-Hop\":\n",
    "      return torch.tensor([0, 1, 0, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Electronic\":\n",
    "      return torch.tensor([0, 0, 1, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Rock\":\n",
    "      return torch.tensor([0, 0, 0, 1, 0, 0]).float()\n",
    "\n",
    "    if label_str == \"Folk\":\n",
    "      return torch.tensor([0, 0, 0, 0, 1, 0]).float()\n",
    "\n",
    "    if label_str == \"Jazz\":\n",
    "      return torch.tensor([0, 0, 0, 0, 0, 1]).float()\n",
    "    \n",
    "  \n",
    "  def _load_audio_list(self):\n",
    "    \n",
    "    audio_path_list = []\n",
    "    \n",
    "    for path, subdirs, files in tqdm(os.walk(self.path), colour=\"magenta\"):\n",
    "      for name in files:\n",
    "          \n",
    "        file_audio_path = os.path.join(path, name)\n",
    "        \n",
    "        audio_path_list.append(file_audio_path)\n",
    "        \n",
    "    return audio_path_list\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = \"extra_small\"\n",
    "DATASET_FOLDER = \"./data/audio\"\n",
    "\n",
    "DATASET_NUM_SAMPLES_PER_SECOND = 8000\n",
    "DATASET_NUM_CHANNELS = 1\n",
    "\n",
    "DATASET_NAME = f\"fma_{DATASET_SIZE}_organized_by_label_resampled_{DATASET_NUM_SAMPLES_PER_SECOND}_rechanneled_{DATASET_NUM_CHANNELS}\"\n",
    "\n",
    "dataset_path = f\"{DATASET_FOLDER}/{DATASET_NAME}\"\n",
    "\n",
    "TRAINING_LOGS_FOLDER = \"./logs\"\n",
    "\n",
    "NORMALIZE_AUDIO = True\n",
    "AUDIO_NUM_FRAMES = 238000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics_json = open(\n",
    "  f\"{dataset_path}_summary_statistics/{DATASET_NAME}_summary_statistics.json\"\n",
    ")\n",
    "\n",
    "summary_statistics_dict = json.load(summary_statistics_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fma_dataset = FMADataset(\n",
    "  path=dataset_path, \n",
    "  normalize_audio=NORMALIZE_AUDIO, \n",
    "  audio_num_frames=AUDIO_NUM_FRAMES,\n",
    "  mean=summary_statistics_dict[\"mean\"], \n",
    "  std=summary_statistics_dict[\"std\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir_if_absent(dir_path):\n",
    "  \n",
    "  # print(\"making dir: \", dir_path)\n",
    "  \n",
    "  if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pca_df = pd.read_json(f\"{dataset_path}_pca/{DATASET_NAME}_pca.json\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "dataset_pca_df[\n",
    "  [\"dataset_pca_1_min_max_scaled\",\"dataset_pca_2_min_max_scaled\"]\n",
    "] = pd.DataFrame(\n",
    "  scaler.fit_transform(\n",
    "    dataset_pca_df[\n",
    "      [\"dataset_pca_1\",\"dataset_pca_2\"]\n",
    "    ].values\n",
    "  ), \n",
    "  columns=[\"dataset_pca_1_min_max_scaled\",\"dataset_pca_2_min_max_scaled\"], \n",
    "  index=dataset_pca_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "  data=dataset_pca_df,\n",
    "  x=\"dataset_pca_1_min_max_scaled\",\n",
    "  y=\"dataset_pca_2_min_max_scaled\",\n",
    "  hue=\"label_scalar\",\n",
    "  palette=\"Dark2\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(fma_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERCENTAGE = 0.7\n",
    "VAL_PERCENTAGE = 0.2\n",
    "\n",
    "full_size = len(fma_dataset)\n",
    "train_size = int(TRAIN_PERCENTAGE * len(fma_dataset))\n",
    "val_size = int(VAL_PERCENTAGE * len(fma_dataset))\n",
    "test_size = full_size - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=torch.Generator().manual_seed(MANUAL_SEED)\n",
    "\n",
    "fma_dataset_train, fma_dataset_val, fma_dataset_test = torch.utils.data.random_split(\n",
    "  fma_dataset, [train_size, val_size, test_size], generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"len(fma_dataset_train): {len(fma_dataset_train)}\")\n",
    "# print(f\"len(fma_dataset_val)  : {len(fma_dataset_val)}\")\n",
    "# print(f\"len(fma_dataset_test) : {len(fma_dataset_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TODO reflect on this and use it as first hypothesis**\n",
    "\n",
    "\"the learning rate and batch size are closely linked â€” small batch sizes perform best with smaller learning rates, while large batch sizes do best on larger learning rates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "data_logs = {\n",
    "  \"dataset_size\": DATASET_SIZE,\n",
    "  \"normalized_audio\": NORMALIZE_AUDIO,\n",
    "  \"audio_num_frames\": AUDIO_NUM_FRAMES,\n",
    "  \"batch_size\": BATCH_SIZE,\n",
    "  \"num_samples_per_second\": DATASET_NUM_SAMPLES_PER_SECOND,\n",
    "  \"num_channels\": DATASET_NUM_CHANNELS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fma_dataloader_train = torch.utils.data.DataLoader(\n",
    "  fma_dataset_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, \n",
    "  generator=generator\n",
    ")\n",
    "fma_dataloader_val = torch.utils.data.DataLoader(\n",
    "  fma_dataset_val, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, \n",
    "  generator=generator\n",
    ")\n",
    "fma_dataloader_test = torch.utils.data.DataLoader(\n",
    "  fma_dataset_test, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, \n",
    "  generator=generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_num_trainable_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(\n",
    "  torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train_id():\n",
    "  return datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_disk(dict, full_path):\n",
    "  with open(full_path, 'w') as fp:\n",
    "    json.dump(dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_ckp(\n",
    "  model, optimizer, ckp_path, train_id, epoch, loss_train, loss_val, loss_test\n",
    "):\n",
    "\n",
    "  model_copy = copy.deepcopy(model)\n",
    "  \n",
    "  full_path_pth = f\"{ckp_path}/{train_id}_epoch_{epoch}.pth\"\n",
    "  full_path_pickle = f\"{ckp_path}/{train_id}_epoch_{epoch}\"\n",
    "  \n",
    "  # print(\"STORING IN: \", full_path_pth)\n",
    "  \n",
    "  make_dir_if_absent(dir_path=\"/\".join(full_path_pth.split('/')[:-1]))\n",
    "  \n",
    "  torch.save(\n",
    "    {\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': model_copy.cpu().state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'loss_train': loss_train,\n",
    "      'loss_val': loss_val,\n",
    "      'loss_test': loss_test,\n",
    "    }, \n",
    "    full_path_pth\n",
    "  )\n",
    "  \n",
    "  torch.save(\n",
    "    model_copy.cpu(), \n",
    "    full_path_pickle\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(ckp_path, perform_loading_sanity_check):\n",
    "\n",
    "  loaded_model = torch.load(ckp_path)\n",
    "\n",
    "  if perform_loading_sanity_check:\n",
    "\n",
    "    loaded_model.eval()\n",
    "\n",
    "    sanity_check_out = loaded_model(torch.rand((16, 1, 238000)))\n",
    "\n",
    "  return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct_preds(outputs, labels):\n",
    "  \n",
    "  output_pred_ind = torch.argmax(outputs, dim=1)\n",
    "  labels_ind = torch.argmax(labels, dim=1)\n",
    "  \n",
    "  matching_mask = (output_pred_ind == labels_ind).float()\n",
    "  \n",
    "  num_correct_preds = matching_mask.sum()\n",
    "  \n",
    "  return num_correct_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "  model, optimizer, criterion,\n",
    "  batch_size, train_dl, val_dl, test_dl, num_epochs, device, \n",
    "  print_freq, ckp_freq, ckp_folder\n",
    "):\n",
    "  \n",
    "  train_id = gen_train_id()\n",
    "  \n",
    "  training_logs = {\n",
    "    \"train_id\": train_id,\n",
    "    \"accuracies\": {},\n",
    "    \"losses\": {}\n",
    "  }\n",
    "  \n",
    "  model = model.to(device)\n",
    "  \n",
    "  pbar_epochs = tqdm(range(num_epochs), colour=\"#9400d3\")\n",
    "  pbar_batches_train = tqdm(\n",
    "    iter(train_dl), colour=\"#4169e1\", leave=False,\n",
    "  )\n",
    "  pbar_batches_val = tqdm(\n",
    "    iter(val_dl), colour=\"#008080\", leave=False,\n",
    "  )\n",
    "  \n",
    "  training_start_time = time.time()\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss_train = 0.0\n",
    "    running_loss_val   = 0.0\n",
    "    running_loss_test  = -1.0\n",
    "    \n",
    "    num_correct_preds_train = 0.0\n",
    "    num_preds_train = 0.0\n",
    "    accuracy_train = 0.0\n",
    "    \n",
    "    num_correct_preds_val = 0.0\n",
    "    num_preds_val = 0.0\n",
    "    accuracy_val = 0.0\n",
    "    \n",
    "    num_correct_preds_test = 0.0\n",
    "    num_preds_test = 0.000000001\n",
    "    accuracy_test = 0.0\n",
    "        \n",
    "    ## BEGIN training step\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    pbar_batches_train.reset()\n",
    "    pbar_batches_val.reset()\n",
    "    \n",
    "    pbar_epochs.set_description(f\"epoch {epoch}\")\n",
    "    pbar_batches_train.set_description(f\"epoch {epoch}\")\n",
    "    pbar_batches_val.set_description  (f\"epoch {epoch}\")\n",
    "    \n",
    "    for batch_x, batch_y in iter(train_dl):\n",
    "\n",
    "      inputs, labels = batch_x, batch_y\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      outputs = outputs.squeeze(-1)\n",
    "      \n",
    "      loss = criterion(outputs, labels)\n",
    "      \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # running_loss_train += loss.item() * inputs.shape[0]\n",
    "      running_loss_train += loss.item() * batch_x.shape[0]\n",
    "      \n",
    "      num_correct_preds_train += get_num_correct_preds(outputs, labels)\n",
    "      num_preds_train += outputs.shape[0]\n",
    "      \n",
    "      pbar_batches_train.update(1)\n",
    "      \n",
    "    \n",
    "    ## END training step\n",
    "    \n",
    "    ## BEGIN validation step\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      \n",
    "      model.eval()\n",
    "      \n",
    "      for batch_x, batch_y in iter(val_dl):\n",
    "\n",
    "        inputs, labels = batch_x, batch_y\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(-1)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # running_loss_val += loss.item() * inputs.shape[0]\n",
    "        running_loss_val += loss.item() * batch_x.shape[0]\n",
    "        \n",
    "        num_correct_preds_val += get_num_correct_preds(outputs, labels)\n",
    "        num_preds_val += outputs.shape[0]\n",
    "        \n",
    "        pbar_batches_val.update(1)\n",
    "        \n",
    "    ## END validation step\n",
    "    \n",
    "    ## BEGIN test step\n",
    "    \n",
    "    if (epoch + 1 == num_epochs):\n",
    "      \n",
    "      pbar_batches_test = tqdm(\n",
    "        iter(test_dl), colour=\"#808000\", leave=False,\n",
    "      )\n",
    "      pbar_batches_test.set_description  (f\"epoch {epoch}\")\n",
    "    \n",
    "      with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for batch_x, batch_y in iter(test_dl):\n",
    "\n",
    "          inputs, labels = batch_x, batch_y\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          \n",
    "          outputs = model(inputs)\n",
    "          outputs = outputs.squeeze(-1)\n",
    "          \n",
    "          loss = criterion(outputs, labels)\n",
    "          \n",
    "          # running_loss_test += loss.item() * inputs.shape[0]\n",
    "          running_loss_test += loss.item() * batch_x.shape[0]\n",
    "          \n",
    "          num_correct_preds_test += get_num_correct_preds(outputs, labels)\n",
    "          num_preds_test += outputs.shape[0]\n",
    "          \n",
    "          pbar_batches_test.update(1)\n",
    "        \n",
    "    ## END test step\n",
    "    \n",
    "    accuracy_train = num_correct_preds_train / num_preds_train\n",
    "    accuracy_val = num_correct_preds_val / num_preds_val\n",
    "    accuracy_test = num_correct_preds_test / num_preds_test\n",
    "    \n",
    "    training_logs[\"accuracies\"][str(epoch)] = {\n",
    "      \"accuracy_train\": accuracy_train.cpu().item(),\n",
    "      \"accuracy_val\": accuracy_val.cpu().item(),\n",
    "    }\n",
    "    training_logs[\"losses\"][str(epoch)] = {\n",
    "      \"loss_train\": running_loss_train,\n",
    "      \"loss_val\": running_loss_val,\n",
    "    }\n",
    "    \n",
    "    pbar_epochs.update(1)\n",
    "    \n",
    "    if ((epoch + 1) % print_freq == 0):  \n",
    "      tqdm.write(\n",
    "        f\"epoch: {epoch + 1}\\n\" + \n",
    "        f\"      train loss: {running_loss_train}, train acc: {accuracy_train}\\n\" + \n",
    "        f\"      val loss  : {running_loss_val}, val acc  : {accuracy_val}\\n\"\n",
    "      )\n",
    "    \n",
    "    if ((epoch + 1) == num_epochs):\n",
    "      tqdm.write(\n",
    "        f\"      test loss : {running_loss_test}, test acc : {accuracy_test}\"\n",
    "      )\n",
    "      \n",
    "      training_logs[\"accuracies\"][str(epoch)][\n",
    "        \"accuracy_test\"\n",
    "      ] = accuracy_test.cpu().item()\n",
    "      \n",
    "      training_logs[\"losses\"][str(epoch)][\n",
    "        \"loss_test\"\n",
    "      ] = running_loss_test\n",
    "      \n",
    "    if (ckp_freq != None and (epoch + 1) % ckp_freq == 0):\n",
    "      \n",
    "      ckp_path = f\"{ckp_folder}/{train_id}\"\n",
    "      \n",
    "      store_ckp(\n",
    "        model=model, optimizer=optimizer, ckp_path=ckp_path, epoch=epoch, \n",
    "        train_id=train_id,\n",
    "        loss_train=running_loss_train, \n",
    "        loss_val=running_loss_val, \n",
    "        loss_test=running_loss_test\n",
    "      )\n",
    "  \n",
    "  training_end_time = time.time()\n",
    "\n",
    "  training_logs[\"training_time_secs\"] = training_end_time - training_start_time\n",
    "\n",
    "  return training_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design motivations\n",
    "\n",
    "First layers --> neural compression layers --> dimensionality reduction to roughly match dimensions of this paper https://arxiv.org/pdf/1703.01789.pdf\n",
    "\n",
    "Mid and final layers --> taken 1:1 from the paper linked above\n",
    "\n",
    "Batch norm placed BEFORE the activation function, as described in the og paper https://arxiv.org/abs/1502.03167 and explained by Bengio in his DL book https://www.deeplearningbook.org/contents/optimization.html in section 8.7.1\n",
    "\n",
    "Dropout placed according to the og paper: https://arxiv.org/pdf/1207.0580.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(\n",
    "    self, \n",
    "    neural_compression_num_layers, \n",
    "    neural_compression_kernel_sizes, neural_compression_strides, \n",
    "    neural_compression_in_channels, neural_compression_num_filters,\n",
    "    neural_compression_pool_sizes, neural_compression_pool_strides,\n",
    "    classification_num_layers, \n",
    "    classification_kernel_sizes, classification_strides, \n",
    "    classification_in_channels, classification_num_filters,\n",
    "    classification_pool_sizes, classification_pool_strides,\n",
    "    dropout_p\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.neural_compression_num_layers = neural_compression_num_layers \n",
    "    self.neural_compression_kernel_sizes = neural_compression_kernel_sizes \n",
    "    self.neural_compression_strides = neural_compression_strides \n",
    "    self.neural_compression_in_channels = neural_compression_in_channels \n",
    "    self.neural_compression_num_filters = neural_compression_num_filters\n",
    "    self.neural_compression_pool_sizes = neural_compression_pool_sizes \n",
    "    self.neural_compression_pool_strides = neural_compression_pool_strides\n",
    "    self.classification_num_layers = classification_num_layers \n",
    "    self.classification_kernel_sizes = classification_kernel_sizes \n",
    "    self.classification_strides = classification_strides \n",
    "    self.classification_in_channels = classification_in_channels \n",
    "    self.classification_num_filters = classification_num_filters\n",
    "    self.classification_pool_sizes = classification_pool_sizes \n",
    "    self.classification_pool_strides = classification_pool_strides\n",
    "    \n",
    "    self.bns = {\n",
    "      \"8\": nn.BatchNorm1d(num_features=8),\n",
    "      \"16\": nn.BatchNorm1d(num_features=16),\n",
    "      \"32\": nn.BatchNorm1d(num_features=32),\n",
    "      \"64\": nn.BatchNorm1d(num_features=64),\n",
    "      \"128\": nn.BatchNorm1d(num_features=128),\n",
    "      \"256\": nn.BatchNorm1d(num_features=256),\n",
    "      \"512\": nn.BatchNorm1d(num_features=512)\n",
    "    }\n",
    "    \n",
    "    self.dropout_p = dropout_p\n",
    "        \n",
    "    ### BEGIN neural compression layers. \n",
    "    ### See above cell for full explanation.\n",
    "    \n",
    "    in_channels = self.neural_compression_in_channels\n",
    "\n",
    "    self.neural_compression_block = nn.Sequential()\n",
    "    \n",
    "    for i in range(neural_compression_num_layers):\n",
    "      \n",
    "      neural_compression_conv_layer = nn.Conv1d(\n",
    "        kernel_size=self.neural_compression_kernel_sizes[i],\n",
    "        stride=self.neural_compression_strides[i],\n",
    "        in_channels=in_channels,\n",
    "        out_channels=self.neural_compression_num_filters[i]\n",
    "      )\n",
    "      \n",
    "      neural_compression_pool_layer = nn.MaxPool1d(\n",
    "        kernel_size=self.neural_compression_pool_sizes[i], \n",
    "        stride=self.neural_compression_pool_strides[i]\n",
    "      )\n",
    "\n",
    "      self.neural_compression_block.add_module(\n",
    "        name=f\"neural_compression_conv_{i}\",\n",
    "        module=neural_compression_conv_layer\n",
    "      )\n",
    "\n",
    "      self.neural_compression_block.add_module(\n",
    "        name=f\"neural_compression_pool_{i}\",\n",
    "        module=neural_compression_pool_layer\n",
    "      )\n",
    "\n",
    "      self.neural_compression_block.add_module(\n",
    "        name=f\"neural_compression_batchnorm_{i}\",\n",
    "        module=self.bns[str(self.neural_compression_num_filters[i])]\n",
    "      )\n",
    "\n",
    "      self.neural_compression_block.add_module(\n",
    "        name=f\"neural_compression_activation_{i}\",\n",
    "        module=nn.ReLU()\n",
    "      )\n",
    "      \n",
    "      in_channels = self.neural_compression_num_filters[i]\n",
    "\n",
    "      \n",
    "    # self.neural_compression_block = nn.Sequential(*neural_compression_layers)\n",
    "    \n",
    "    ### END Neural compression layers\n",
    "    \n",
    "    ### BEGIN classification layers. \n",
    "    ### See above cell for full explanation.\n",
    "    \n",
    "    in_channels = self.classification_in_channels\n",
    "\n",
    "    self.classification_block = nn.Sequential()\n",
    "    \n",
    "    for i in range(classification_num_layers):\n",
    "      classification_conv_layer = nn.Conv1d(\n",
    "        kernel_size=self.classification_kernel_sizes[i],\n",
    "        stride=self.classification_strides[i],\n",
    "        in_channels=in_channels,\n",
    "        out_channels=self.classification_num_filters[i]\n",
    "      )\n",
    "      \n",
    "      classification_pooling_layer = nn.MaxPool1d(\n",
    "        kernel_size=self.classification_pool_sizes[i],\n",
    "        stride=self.classification_pool_strides[i],\n",
    "      )\n",
    "      \n",
    "      in_channels = self.classification_num_filters[i]\n",
    "      \n",
    "      self.classification_block.add_module(\n",
    "        name=f\"classification_conv_{i}\", module=classification_conv_layer\n",
    "      )\n",
    "      \n",
    "      self.classification_block.add_module(\n",
    "        name=f\"classification_pool_{i}\", module=classification_pooling_layer\n",
    "      )\n",
    "      \n",
    "      if (i < classification_num_layers - 1):\n",
    "        \n",
    "        self.classification_block.add_module(\n",
    "          name=f\"classification_batchnorm_{i}\", \n",
    "          module=self.bns[str(self.classification_num_filters[i])]\n",
    "        )\n",
    "      \n",
    "      if (i < classification_num_layers - 1):\n",
    "        \n",
    "        self.classification_block.add_module(\n",
    "          name=f\"classification_activation_{i}\", module=nn.ReLU()\n",
    "        )\n",
    "\n",
    "      else:\n",
    "        \n",
    "        self.classification_block.add_module(\n",
    "          name=f\"classification_activation_{i}\", module=nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "      if (i == classification_num_layers - 2):\n",
    "\n",
    "        self.classification_block.add_module(\n",
    "          name=f\"classification_dropout_{i}\", \n",
    "          module=nn.Dropout(p=self.dropout_p)\n",
    "        )\n",
    "    \n",
    "    ### END classification layers. \n",
    "    ### See above cell for full explanation.\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.neural_compression_block(x)\n",
    "    \n",
    "    x = self.classification_block(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "  def get_model_setup(self):\n",
    "    \n",
    "    return {\n",
    "      \"neural_compression_num_layers\": self.neural_compression_num_layers, \n",
    "      \"neural_compression_kernel_sizes\": self.neural_compression_kernel_sizes, \n",
    "      \"neural_compression_strides\": self.neural_compression_strides, \n",
    "      \"neural_compression_in_channels\": self.neural_compression_in_channels, \n",
    "      \"neural_compression_num_filters\": self.neural_compression_num_filters,\n",
    "      \"neural_compression_pool_sizes\": self.neural_compression_pool_sizes, \n",
    "      \"neural_compression_pool_strides\": self.neural_compression_pool_strides,\n",
    "      \"classification_num_layers\": self.classification_num_layers, \n",
    "      \"classification_kernel_sizes\": self.classification_kernel_sizes, \n",
    "      \"classification_strides\": self.classification_strides, \n",
    "      \"classification_in_channels\": self.classification_in_channels, \n",
    "      \"classification_num_filters\": self.classification_num_filters,\n",
    "      \"classification_pool_sizes\": self.classification_pool_sizes, \n",
    "      \"classification_pool_strides\": self.classification_pool_strides,\n",
    "      \"dropout_p\": self.dropout_p\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_params(model):\n",
    "\n",
    "  model_copy = copy.deepcopy(model)\n",
    "\n",
    "  conv_layer_params = {}\n",
    "\n",
    "  for sequential_block_name, sequential_block in model_copy.named_children():\n",
    "    # print(sequential_block_name)\n",
    "\n",
    "    for layer_name, layer in sequential_block.named_children():\n",
    "      \n",
    "      # only interested in convolutional layers, NOT in batchnorm or other kinds\n",
    "      if (\"conv\" in layer_name):\n",
    "      \n",
    "        for weight_or_bias, layer_params in layer.named_parameters():\n",
    "          \n",
    "          # only interested in weights, we do NOT care about biases\n",
    "          if (\"weight\" in weight_or_bias):\n",
    "            # print(f\"    {layer_name}.{weight_or_bias}\")\n",
    "            # print(f\"    {layer_params.data.shape}\")\n",
    "            conv_layer_params[layer_name] = layer_params.data\n",
    "\n",
    "  return conv_layer_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_params_layered_flattened(conv_params_layered):\n",
    "  conv_params_layered_flattened = {}\n",
    "\n",
    "  for layer_name in conv_params_layered:\n",
    "    conv_params_layered_flattened[layer_name] = conv_params_layered[layer_name].reshape((-1, 3))\n",
    "\n",
    "  return conv_params_layered_flattened\n",
    "\n",
    "def get_conv_params_layered_flattened_stacked(conv_params_layered_flattened):\n",
    "\n",
    "  return torch.cat(tuple(conv_params_layered_flattened.values()), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conv_params_layered_flattened_space(\n",
    "  conv_params_layered_flattened, fig_base_path, show_plot\n",
    "):\n",
    "  \n",
    "  pca = PCA(n_components=2)\n",
    "\n",
    "  conv_params_layered_flattened_pca = {}\n",
    "  conv_params_layered_flattened_pca_l_2_norms = {}\n",
    "\n",
    "\n",
    "  for layer_name in conv_params_layered_flattened:\n",
    "    conv_params_layered_flattened_pca[layer_name] = pca.fit_transform(\n",
    "      conv_params_layered_flattened[layer_name]\n",
    "    )\n",
    "\n",
    "    conv_params_layered_flattened_pca_l_2_norms[layer_name] = np.linalg.norm(\n",
    "      x=conv_params_layered_flattened_pca[layer_name], ord=2, axis=1\n",
    "    )\n",
    "\n",
    "    plt.figure()\n",
    "    scatter_plot = sns.scatterplot(\n",
    "      x=conv_params_layered_flattened_pca[layer_name][:, 0],\n",
    "      y=conv_params_layered_flattened_pca[layer_name][:, 1],\n",
    "      hue=conv_params_layered_flattened_pca_l_2_norms[layer_name], \n",
    "      palette=\"mako\"\n",
    "    )\n",
    "    scatter_plot.set(title=f\"CNN layer: {layer_name}\")\n",
    "\n",
    "    figure = scatter_plot.get_figure()\n",
    "    figure.savefig(\n",
    "      f\"{fig_base_path}_{layer_name}.png\", dpi=400, bbox_inches='tight'\n",
    "    )\n",
    "\n",
    "    if not show_plot:\n",
    "      plt.clf()\n",
    "\n",
    "  print()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conv_params_space(conv_params, fig_path):\n",
    "\n",
    "  pca = PCA(n_components=2)\n",
    "\n",
    "  conv_params_pca = pca.fit_transform(conv_params)\n",
    "\n",
    "  scatter_plot = sns.scatterplot(\n",
    "    x=conv_params_pca[:, 0],\n",
    "    y=conv_params_pca[:, 1],\n",
    "    hue=np.zeros_like(conv_params_pca[:, 1]),\n",
    "    palette=\"deep6\",\n",
    "    legend=False\n",
    "  )\n",
    "  \n",
    "  scatter_plot.set(title=f\"CNN filter space (after PCA, all conv layers)\")\n",
    "\n",
    "  scatter_plot_figure = scatter_plot.get_figure()\n",
    "  scatter_plot_figure.savefig(fig_path, dpi=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conv_params_layered_flattened_distribs(\n",
    "  conv_params_layered_flattened, fig_base_path, show_plot\n",
    "):\n",
    "  pca = PCA(n_components=1)\n",
    "\n",
    "  conv_params_layered_flattened_pca = {}\n",
    "\n",
    "  for layer_name in conv_params_layered_flattened:\n",
    "    conv_params_layered_flattened_pca[layer_name] = pca.fit_transform(\n",
    "      conv_params_layered_flattened[layer_name]\n",
    "    )\n",
    "\n",
    "    distrib_plot = sns.displot(\n",
    "      x=conv_params_layered_flattened_pca[layer_name].squeeze(1),\n",
    "      color=\"green\"\n",
    "    )\n",
    "    \n",
    "    distrib_plot.set(\n",
    "      title=f\"CNN filter distribution (after PCA) for layer {layer_name}\"\n",
    "    )\n",
    "\n",
    "    figure = distrib_plot.fig\n",
    "    figure.savefig(\n",
    "      f\"{fig_base_path}_{layer_name}.png\", dpi=400, bbox_inches='tight'\n",
    "    )\n",
    "\n",
    "    if not show_plot:\n",
    "      plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conv_params_distrib(conv_params, fig_path):\n",
    "\n",
    "  pca = PCA(n_components=1)\n",
    "\n",
    "  conv_params_pca = pca.fit_transform(conv_params)\n",
    "  conv_params_pca = conv_params_pca.squeeze(1)\n",
    "\n",
    "  distrib_plot = sns.displot(\n",
    "    x=conv_params_pca,\n",
    "    color=\"dodgerblue\"\n",
    "  ).set(title=f\"CNN filter distribution (after PCA, all conv layers)\")\n",
    "\n",
    "  figure = distrib_plot.fig    \n",
    "  figure.savefig(fig_path, dpi=400, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(stats):\n",
    "  epochs = stats[\"training_logs\"][\"losses\"].keys()\n",
    "  \n",
    "  loss_train = [\n",
    "    j[\"loss_train\"] for j in stats[\"training_logs\"][\"losses\"].values()\n",
    "  ]\n",
    "  \n",
    "  loss_val = [j[\"loss_val\"] for j in stats[\"training_logs\"][\"losses\"].values()]\n",
    "\n",
    "  sns.lineplot(\n",
    "    x=epochs,\n",
    "    y=loss_train,\n",
    "    legend=\"full\",\n",
    "    label=\"train loss\"\n",
    "  )\n",
    "\n",
    "  sns.lineplot(\n",
    "    x=epochs,\n",
    "    y=loss_val,\n",
    "    legend=\"full\",\n",
    "    label=\"val loss\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_compression_num_layers   = 3\n",
    "neural_compression_kernel_sizes = [ 3,  3,  3]\n",
    "neural_compression_strides      = [ 3,  2,  2]\n",
    "neural_compression_num_filters  = [ 8, 16, 32]\n",
    "neural_compression_in_channels  = 1\n",
    "neural_compression_pool_sizes   = [3, 3, 3]\n",
    "neural_compression_pool_strides = [1, 1, 2]\n",
    "\n",
    "classification_num_layers    = 4\n",
    "classification_kernel_sizes  = [ 3,  3,  3, 3]\n",
    "classification_strides       = [ 3,  3,  3, 3]\n",
    "classification_in_channels   = neural_compression_num_filters[-1]\n",
    "classification_num_filters   = [ 32, 64, 128, 6]\n",
    "classification_pool_sizes    = [3, 3, 3, 3]\n",
    "classification_pool_strides  = [3, 3, 3, 3]\n",
    "\n",
    "DROPOUT_P = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_attempt_1 = CNN(\n",
    "  # neural compression layers parameters\n",
    "  neural_compression_num_layers=neural_compression_num_layers,\n",
    "  neural_compression_kernel_sizes=neural_compression_kernel_sizes, \n",
    "  neural_compression_strides=neural_compression_strides, \n",
    "  neural_compression_in_channels=neural_compression_in_channels, \n",
    "  neural_compression_num_filters=neural_compression_num_filters,\n",
    "  neural_compression_pool_sizes=neural_compression_pool_sizes,\n",
    "  neural_compression_pool_strides=neural_compression_pool_strides,\n",
    "  # classification layers parameters\n",
    "  classification_num_layers=classification_num_layers,\n",
    "  classification_kernel_sizes=classification_kernel_sizes, \n",
    "  classification_strides=classification_strides, \n",
    "  classification_in_channels=classification_in_channels, \n",
    "  classification_num_filters=classification_num_filters,\n",
    "  classification_pool_sizes=classification_pool_sizes,\n",
    "  classification_pool_strides=classification_pool_strides,\n",
    "  dropout_p=DROPOUT_P\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = cnn_attempt_1(torch.rand((16, 1, 238000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_num_trainable_parameters(cnn_attempt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-6\n",
    "OPTIMIZER = \"SGD\"\n",
    "\n",
    "if OPTIMIZER == \"SGD\":\n",
    "  optimizer = optim.SGD(\n",
    "    cnn_attempt_1.parameters(), \n",
    "    lr=LR, \n",
    "    momentum=MOMENTUM,\n",
    "    nesterov=True,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    "  )\n",
    "  pass\n",
    "\n",
    "elif OPTIMIZER == \"Adam\":\n",
    "\n",
    "  optimizer = optim.Adam(\n",
    "    cnn_attempt_1.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    "  )\n",
    "  pass\n",
    "\n",
    "optimizer_parameters = {\n",
    "  \"optimizer\": OPTIMIZER,\n",
    "  \"lr\": LR, \n",
    "  \"momentum\": MOMENTUM, \n",
    "  \"weight_decay\": WEIGHT_DECAY\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "PRINT_FREQ = 1\n",
    "CKP_FREQ = 2\n",
    "\n",
    "PERFORM_TRAINING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERFORM_TRAINING:\n",
    "  \n",
    "  training_logs = train_model(\n",
    "    model=cnn_attempt_1, optimizer=optimizer, criterion=criterion,\n",
    "    batch_size=BATCH_SIZE, train_dl=fma_dataloader_train, \n",
    "    val_dl=fma_dataloader_val,test_dl=fma_dataloader_test,\n",
    "    num_epochs=NUM_EPOCHS, device=device,\n",
    "    print_freq=PRINT_FREQ,\n",
    "    ckp_folder=TRAINING_LOGS_FOLDER, ckp_freq=CKP_FREQ\n",
    "  )\n",
    "\n",
    "  stats = {\n",
    "    \"data_logs\": data_logs,\n",
    "    \"optimizer_parameters\": optimizer_parameters,\n",
    "    \"model_setup\": cnn_attempt_1.get_model_setup(),\n",
    "    \"training_logs\": training_logs,\n",
    "  }\n",
    "\n",
    "  train_id = training_logs[\"train_id\"]\n",
    "  save_dict_to_disk(\n",
    "    dict=stats,\n",
    "    full_path=f\"{TRAINING_LOGS_FOLDER}/{train_id}/{train_id}.json\"\n",
    "  )\n",
    "\n",
    "else:\n",
    "\n",
    "  train_id = \"05_12_2022_11_52_21\"\n",
    "\n",
    "  EPOCH_TO_LOAD = 3\n",
    "  CKP_PATH = f\"./logs/{train_id}/{train_id}_epoch_{EPOCH_TO_LOAD}\"\n",
    "  PERFORM_LOADING_SANITY_CHECK = True\n",
    "\n",
    "  cnn_attempt_1 = load_ckp(\n",
    "    ckp_path=CKP_PATH,\n",
    "    perform_loading_sanity_check=PERFORM_LOADING_SANITY_CHECK\n",
    "  )\n",
    "\n",
    "  STATS_PATH = f\"./logs/{train_id}/{train_id}.json\"\n",
    "\n",
    "  stats_json = open(STATS_PATH)\n",
    "\n",
    "  stats = json.load(stats_json)\n",
    "\n",
    "conv_params_layered = get_conv_params(cnn_attempt_1)\n",
    "\n",
    "conv_params_layered_flattened = get_conv_params_layered_flattened(\n",
    "  conv_params_layered=conv_params_layered\n",
    ")\n",
    "\n",
    "conv_params = torch.cat(list(conv_params_layered_flattened.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_PARAMS_LAYERED_FLATTENED_DISTRIB_FIG_FOLDER = f\"./logs/{train_id}/{train_id}_conv_params_layered_flattened_distribs\"\n",
    "CONV_PARAMS_LAYERED_FLATTENED_DISTRIB_SHOW_PLOT=False\n",
    "\n",
    "plot_conv_params_layered_flattened_distribs(\n",
    "  conv_params_layered_flattened=conv_params_layered_flattened,\n",
    "  fig_base_path=CONV_PARAMS_LAYERED_FLATTENED_DISTRIB_FIG_FOLDER,\n",
    "  show_plot=CONV_PARAMS_LAYERED_FLATTENED_DISTRIB_SHOW_PLOT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_PARAMS_DISTRIB_FIG_PATH = f\"./logs/{train_id}/{train_id}_conv_param_distrib.png\"\n",
    "plot_conv_params_distrib(conv_params, fig_path=CONV_PARAMS_DISTRIB_FIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_PARAMS_LAYERED_FLATTENED_SPACE_FIG_FOLDER = f\"./logs/{train_id}/{train_id}_conv_params_layered_flattened_space\"\n",
    "CONV_PARAMS_LAYERED_FLATTENED_SPACE_SHOW_PLOT=False\n",
    "\n",
    "plot_conv_params_layered_flattened_space(\n",
    "  conv_params_layered_flattened=conv_params_layered_flattened,\n",
    "  fig_base_path=CONV_PARAMS_LAYERED_FLATTENED_SPACE_FIG_FOLDER,\n",
    "  show_plot=CONV_PARAMS_LAYERED_FLATTENED_SPACE_SHOW_PLOT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_PARAMS_SPACE_FIG_PATH = f\"./logs/{train_id}/{train_id}_conv_param_space.png\"\n",
    "plot_conv_params_space(conv_params, fig_path=CONV_PARAMS_SPACE_FIG_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
