{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConverter(Dataset):\n",
    "    def __init__(\n",
    "      self, path: str, use_spectrogram: bool, normalize_audio: bool, \n",
    "      audio_num_frames: int, audio_hop_length: int\n",
    "    ):\n",
    "      self.path = path\n",
    "      self.use_spectrogram = use_spectrogram\n",
    "      self.normalize_audio = normalize_audio\n",
    "      self.audio_num_frames = audio_num_frames\n",
    "      self.audio_hop_length = audio_hop_length\n",
    "      \n",
    "      # TODO load the raw dataset only once, then use it in another class/cell\n",
    "      # to perform all the desired \"compressions\"/samplings/whatever \n",
    "      self.data_waveform, self.data_mel_spectrogram = self.load_audio_data()\n",
    "      \n",
    "      # TODO export self.data_waveform to disk\n",
    "      # TODO export self.data_spectrogram to disk\n",
    "  \n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      return self.data_waveform[idx], self.data_mel_spectrogram[idx]\n",
    "    \n",
    "    def label_from_str_to_one_hot(self, label_str: str): \n",
    "      \n",
    "      if label_str == \"Pop\":\n",
    "        return torch.tensor([1, 0, 0, 0, 0, 0])\n",
    "      \n",
    "      if label_str == \"Hip-Hop\":\n",
    "        return torch.tensor([0, 1, 0, 0, 0, 0])\n",
    "      \n",
    "      if label_str == \"Electronic\":\n",
    "        return torch.tensor([0, 0, 1, 0, 0, 0])\n",
    "      \n",
    "      if label_str == \"Rock\":\n",
    "        return torch.tensor([0, 0, 0, 1, 0, 0])\n",
    "\n",
    "      if label_str == \"Folk\":\n",
    "        return torch.tensor([0, 0, 0, 0, 1, 0])\n",
    "\n",
    "      if label_str == \"Jazz\":\n",
    "        return torch.tensor([0, 0, 0, 0, 0, 1])\n",
    "      \n",
    "    def get_mel_spectrogram(self, waveform, sample_rate):\n",
    "      \n",
    "      # TODO save the MelSpectrogram object in the class constructor in order\n",
    "      # to avoid to re-init it every time\n",
    "      \n",
    "      n_fft = 1024\n",
    "      win_length = None\n",
    "      # hop_length = 512\n",
    "      hop_length = 1\n",
    "      n_mels = 128\n",
    "\n",
    "      mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm='slaney',\n",
    "        onesided=True,\n",
    "        n_mels=n_mels,\n",
    "        mel_scale=\"htk\",\n",
    "      )\n",
    "\n",
    "      return mel_spectrogram_transform(waveform)\n",
    "    \n",
    "    def load_audio_data(self):\n",
    "      \n",
    "      audio_file_list = []\n",
    "      data_waveform, data_mel_spectrogram = [], []\n",
    "\n",
    "      for path, subdirs, files in tqdm(os.walk(self.path), colour=\"teal\"):\n",
    "        for name in tqdm(files, colour=\"turquoise\"):\n",
    "            \n",
    "            file_audio_path = os.path.join(path, name)\n",
    "            \n",
    "            try:\n",
    "              waveform, sample_rate = torchaudio.load(\n",
    "                file_audio_path, normalize=self.normalize_audio,\n",
    "                num_frames=self.audio_num_frames\n",
    "              )\n",
    "            except:\n",
    "              print(f\"Got an error while loading {file_audio_path}\")\n",
    "              continue\n",
    "            \n",
    "            waveform = torchaudio.functional.resample(\n",
    "              waveform, orig_freq=self.audio_hop_length, new_freq=1\n",
    "              # orig_freq=sample_rate, \n",
    "              # new_freq=sample_rate / self.audio_hop_length\n",
    "            )\n",
    "            \n",
    "            label = file_audio_path.split(\"/\")[-2]\n",
    "            label_one_hot = self.label_from_str_to_one_hot(label)\n",
    "            \n",
    "            temp_dict_waveform = {\n",
    "              \"label\": label_one_hot,\n",
    "              \"waveform\": waveform\n",
    "            }\n",
    "            \n",
    "            temp_dict_mel_spectrogram = {\n",
    "              \"label\": label_one_hot,\n",
    "              \"mel_spectrogram\": self.get_mel_spectrogram(\n",
    "                waveform, sample_rate\n",
    "              )\n",
    "            }\n",
    "            \n",
    "            data_waveform.append(temp_dict_waveform)\n",
    "            data_mel_spectrogram.append(temp_dict_mel_spectrogram)\n",
    "            \n",
    "      return data_waveform, data_mel_spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa67d557e8ad48868e49462507e1dba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0d4dfef1604345974844083b74ef71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067f665e3b4646adb5b427970c38b417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8a5c3aef0a40dead2245a0ea13dd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5786e3c819bf476880e6501c424609f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11b17f295cb4b4289f104754dc10671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb7be67ea5447ae9c60cc941e0294e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882d7a19721e47f498257993bac958f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_converter = DatasetConverter(\n",
    "  path=\"./data/fma_extra_small_organized_by_label/\", \n",
    "  # path=\"./data/fma_large_6_top_level_downsampled_organized_by_label/\", \n",
    "  # KEEP THIS SET TO TRUE WHILE EXPORTING THE DATASETS IN THE \"COMPRESSED\" FORMAT\n",
    "  use_spectrogram=True, \n",
    "  normalize_audio=True, audio_num_frames=1320000, \n",
    "  audio_hop_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 2580])\n",
      "torch.Size([2, 2579])\n"
     ]
    }
   ],
   "source": [
    "print(dataset_converter.data_mel_spectrogram[3][\"mel_spectrogram\"].shape)\n",
    "print(dataset_converter.data_waveform[3][\"waveform\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
