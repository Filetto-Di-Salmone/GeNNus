{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms\n",
    "\n",
    "import torchvision\n",
    "\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import gc\n",
    "\n",
    "MANUAL_SEED = 69\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "  \n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "import copy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir_if_absent(dir_path):\n",
    "  \n",
    "  if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMADataset(Dataset):\n",
    "\n",
    "  def __init__(\n",
    "    self, path, transforms, data_type, mean, std\n",
    "  ):\n",
    "    self.path = path\n",
    "    self.transforms = transforms\n",
    "    self.data_type = data_type,\n",
    "    self.mean = mean\n",
    "    self.std = std\n",
    "    \n",
    "    self.data_paths = self._load_audio_list()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data_paths)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    data = (torch.load(self.data_paths[idx]) - self.mean) / self.std\n",
    "\n",
    "    label_one_hot = self._label_from_str_to_one_hot(\n",
    "      self.data_paths[idx].split(\"/\")[-2]\n",
    "    )\n",
    "\n",
    "    return data, label_one_hot\n",
    "  \n",
    "  def _label_from_str_to_one_hot(self, label_str: str): \n",
    "  \n",
    "    if label_str == \"Pop\":\n",
    "      return torch.tensor([1, 0, 0, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Hip-Hop\":\n",
    "      return torch.tensor([0, 1, 0, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Electronic\":\n",
    "      return torch.tensor([0, 0, 1, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Rock\":\n",
    "      return torch.tensor([0, 0, 0, 1, 0, 0]).float()\n",
    "\n",
    "    if label_str == \"Folk\":\n",
    "      return torch.tensor([0, 0, 0, 0, 1, 0]).float()\n",
    "\n",
    "    if label_str == \"Jazz\":\n",
    "      return torch.tensor([0, 0, 0, 0, 0, 1]).float()\n",
    "    \n",
    "  \n",
    "  def _load_audio_list(self):\n",
    "    \n",
    "    audio_path_list = []\n",
    "    \n",
    "    for path, subdirs, files in os.walk(self.path):\n",
    "      for name in files:\n",
    "          \n",
    "        file_audio_path = os.path.join(path, name)\n",
    "        \n",
    "        audio_path_list.append(file_audio_path)\n",
    "        \n",
    "    return audio_path_list\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = \"xs\"\n",
    "DATASET_TYPE = \"waveform\"\n",
    "DATASET_FOLDER = f\"./data/{DATASET_TYPE}\"\n",
    "\n",
    "DATASET_NUM_SAMPLES_PER_SECOND = 8000\n",
    "DATASET_NUM_CHANNELS = 1\n",
    "\n",
    "DATASET_NAME = f\"fma_{DATASET_SIZE}_resampled_{DATASET_NUM_SAMPLES_PER_SECOND}_rechanneled_{DATASET_NUM_CHANNELS}\"\n",
    "\n",
    "dataset_path = f\"{DATASET_FOLDER}/{DATASET_NAME}\"\n",
    "\n",
    "SUMMARY_STATISTICS_PATH = f\"./data/summary_statistics/{DATASET_NAME}/{DATASET_NAME}_summary_statistics.json\"\n",
    "\n",
    "TRAINING_LOGS_FOLDER = \"./logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics_json = open(SUMMARY_STATISTICS_PATH)\n",
    "\n",
    "summary_statistics_dict = json.load(summary_statistics_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fma_data_transforms = torch.nn.Sequential(\n",
    "  # torchvision.transforms.Normalize(\n",
    "  #   summary_statistics_dict[f\"{DATASET_TYPE}_mean\"],\n",
    "  #   summary_statistics_dict[f\"{DATASET_TYPE}_std\"]\n",
    "  # )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fma_dataset = FMADataset(\n",
    "  path=dataset_path, \n",
    "  transforms=fma_data_transforms,\n",
    "  data_type=DATASET_TYPE,\n",
    "  mean=summary_statistics_dict[f\"{DATASET_TYPE}_mean\"],\n",
    "  std=summary_statistics_dict[f\"{DATASET_TYPE}_std\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data, label_one_hot in fma_dataset:\n",
    "#   # print(data.shape, label_one_hot)\n",
    "#   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERCENTAGE = 0.7\n",
    "VAL_PERCENTAGE = 0.2\n",
    "\n",
    "full_size = len(fma_dataset)\n",
    "train_size = int(TRAIN_PERCENTAGE * len(fma_dataset))\n",
    "val_size = int(VAL_PERCENTAGE * len(fma_dataset))\n",
    "test_size = full_size - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=torch.Generator().manual_seed(MANUAL_SEED)\n",
    "\n",
    "fma_dataset_train, fma_dataset_val, fma_dataset_test = torch.utils.data.random_split(\n",
    "  fma_dataset, [train_size, val_size, test_size], generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"len(fma_dataset_train): {len(fma_dataset_train)}\")\n",
    "# print(f\"len(fma_dataset_val)  : {len(fma_dataset_val)}\")\n",
    "# print(f\"len(fma_dataset_test) : {len(fma_dataset_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TODO reflect on this and use it as first hypothesis**\n",
    "\n",
    "\"the learning rate and batch size are closely linked â€” small batch sizes perform best with smaller learning rates, while large batch sizes do best on larger learning rates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "data_logs = {\n",
    "  \"data_type\": DATASET_TYPE,\n",
    "  \"dataset_size\": DATASET_SIZE,\n",
    "  \"batch_size\": BATCH_SIZE,\n",
    "  \"num_samples_per_second\": DATASET_NUM_SAMPLES_PER_SECOND,\n",
    "  \"num_channels\": DATASET_NUM_CHANNELS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fma_dataloader_train = torch.utils.data.DataLoader(\n",
    "  fma_dataset_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, \n",
    "  generator=generator\n",
    ")\n",
    "fma_dataloader_val = torch.utils.data.DataLoader(\n",
    "  fma_dataset_val, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, \n",
    "  generator=generator\n",
    ")\n",
    "fma_dataloader_test = torch.utils.data.DataLoader(\n",
    "  fma_dataset_test, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, \n",
    "  generator=generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_num_trainable_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "print(\n",
    "  torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train_id():\n",
    "  return datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_disk(dict, full_path):\n",
    "\n",
    "  make_dir_if_absent(\"/\".join(full_path.split(\"/\")[:-1]))\n",
    "\n",
    "  with open(full_path, 'w') as fp:\n",
    "    json.dump(dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_ckp(\n",
    "  model, optimizer, ckp_path, train_id, epoch, loss_train, loss_val, loss_test\n",
    "):\n",
    "\n",
    "  model_copy = copy.deepcopy(model)\n",
    "  \n",
    "  full_path_pth = f\"{ckp_path}/{train_id}_epoch_{epoch}.pth\"\n",
    "  full_path_pickle = f\"{ckp_path}/{train_id}_epoch_{epoch}\"\n",
    "  \n",
    "  # print(\"STORING IN: \", full_path_pth)\n",
    "  \n",
    "  make_dir_if_absent(dir_path=\"/\".join(full_path_pth.split('/')[:-1]))\n",
    "  \n",
    "  torch.save(\n",
    "    {\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': model_copy.cpu().state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'loss_train': loss_train,\n",
    "      'loss_val': loss_val,\n",
    "      'loss_test': loss_test,\n",
    "    }, \n",
    "    full_path_pth\n",
    "  )\n",
    "  \n",
    "  torch.save(\n",
    "    model_copy.cpu(), \n",
    "    full_path_pickle\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(ckp_path, perform_loading_sanity_check):\n",
    "\n",
    "  loaded_model = torch.load(ckp_path)\n",
    "\n",
    "  if perform_loading_sanity_check:\n",
    "\n",
    "    loaded_model.eval()\n",
    "\n",
    "    sanity_check_out = loaded_model(torch.rand((16, 1, 238000)))\n",
    "\n",
    "  return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct_preds(outputs, labels):\n",
    "  \n",
    "  output_pred_ind = torch.argmax(outputs, dim=1)\n",
    "  labels_ind = torch.argmax(labels, dim=1)\n",
    "  \n",
    "  matching_mask = (output_pred_ind == labels_ind).float()\n",
    "  \n",
    "  num_correct_preds = matching_mask.sum()\n",
    "  \n",
    "  return num_correct_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "  model, optimizer, criterion,\n",
    "  batch_size, train_dl, val_dl, test_dl, \n",
    "  num_epochs, \n",
    "  device, \n",
    "  print_freq, ckp_freq, \n",
    "  ckp_folder\n",
    "):\n",
    "  \n",
    "  train_id = gen_train_id()\n",
    "  \n",
    "  training_logs = {\n",
    "    \"train_id\": train_id,\n",
    "    \"accuracies\": {},\n",
    "    \"losses\": {}\n",
    "  }\n",
    "  \n",
    "  model = model.to(device)\n",
    "  \n",
    "  pbar_epochs = tqdm(range(num_epochs), colour=\"#9400d3\")\n",
    "  pbar_batches_train = tqdm(\n",
    "    iter(train_dl), colour=\"#4169e1\", leave=False,\n",
    "  )\n",
    "  pbar_batches_val = tqdm(\n",
    "    iter(val_dl), colour=\"#008080\", leave=False,\n",
    "  )\n",
    "  \n",
    "  training_start_time = time.time()\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss_train = 0.0\n",
    "    running_loss_val   = 0.0\n",
    "    running_loss_test  = -1.0\n",
    "    \n",
    "    num_correct_preds_train = 0.0\n",
    "    num_preds_train = 0.0\n",
    "    accuracy_train = 0.0\n",
    "    \n",
    "    num_correct_preds_val = 0.0\n",
    "    num_preds_val = 0.0\n",
    "    accuracy_val = 0.0\n",
    "    \n",
    "    num_correct_preds_test = 0.0\n",
    "    num_preds_test = 0.000000001\n",
    "    accuracy_test = 0.0\n",
    "        \n",
    "    ## BEGIN training step\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    pbar_batches_train.reset()\n",
    "    pbar_batches_val.reset()\n",
    "    \n",
    "    pbar_epochs.set_description(f\"epoch {epoch}\")\n",
    "    pbar_batches_train.set_description(f\"epoch {epoch}\")\n",
    "    pbar_batches_val.set_description  (f\"epoch {epoch}\")\n",
    "    \n",
    "    for batch_x, batch_y in iter(train_dl):\n",
    "\n",
    "      inputs, labels = batch_x, batch_y\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      outputs = outputs.squeeze(-1)\n",
    "      \n",
    "      loss = criterion(outputs, labels)\n",
    "      \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss_train += loss.item() * batch_x.shape[0]\n",
    "      \n",
    "      num_correct_preds_train += get_num_correct_preds(outputs, labels)\n",
    "      num_preds_train += outputs.shape[0]\n",
    "      \n",
    "      pbar_batches_train.update(1)\n",
    "      \n",
    "    \n",
    "    ## END training step\n",
    "    \n",
    "    ## BEGIN validation step\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      \n",
    "      model.eval()\n",
    "      \n",
    "      for batch_x, batch_y in iter(val_dl):\n",
    "\n",
    "        inputs, labels = batch_x, batch_y\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(-1)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss_val += loss.item() * batch_x.shape[0]\n",
    "        \n",
    "        num_correct_preds_val += get_num_correct_preds(outputs, labels)\n",
    "        num_preds_val += outputs.shape[0]\n",
    "        \n",
    "        pbar_batches_val.update(1)\n",
    "        \n",
    "    ## END validation step\n",
    "    \n",
    "    ## BEGIN test step\n",
    "    \n",
    "    if (epoch + 1 == num_epochs):\n",
    "      \n",
    "      pbar_batches_test = tqdm(\n",
    "        iter(test_dl), colour=\"#808000\", leave=False,\n",
    "      )\n",
    "      pbar_batches_test.set_description  (f\"epoch {epoch}\")\n",
    "    \n",
    "      with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for batch_x, batch_y in iter(test_dl):\n",
    "\n",
    "          inputs, labels = batch_x, batch_y\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          \n",
    "          outputs = model(inputs)\n",
    "          outputs = outputs.squeeze(-1)\n",
    "          \n",
    "          loss = criterion(outputs, labels)\n",
    "          \n",
    "          running_loss_test += loss.item() * batch_x.shape[0]\n",
    "          \n",
    "          num_correct_preds_test += get_num_correct_preds(outputs, labels)\n",
    "          num_preds_test += outputs.shape[0]\n",
    "          \n",
    "          pbar_batches_test.update(1)\n",
    "        \n",
    "    ## END test step\n",
    "    \n",
    "    accuracy_train = num_correct_preds_train / num_preds_train\n",
    "    accuracy_val = num_correct_preds_val / num_preds_val\n",
    "    accuracy_test = num_correct_preds_test / num_preds_test\n",
    "    \n",
    "    training_logs[\"accuracies\"][str(epoch)] = {\n",
    "      \"accuracy_train\": accuracy_train.cpu().item(),\n",
    "      \"accuracy_val\": accuracy_val.cpu().item(),\n",
    "    }\n",
    "    training_logs[\"losses\"][str(epoch)] = {\n",
    "      \"loss_train\": running_loss_train,\n",
    "      \"loss_val\": running_loss_val,\n",
    "    }\n",
    "    \n",
    "    pbar_epochs.update(1)\n",
    "    \n",
    "    if ((epoch + 1) % print_freq == 0):  \n",
    "      tqdm.write(\n",
    "        f\"epoch: {epoch + 1}\\n\" + \n",
    "        f\"      train loss: {running_loss_train}, train acc: {accuracy_train}\\n\" + \n",
    "        f\"      val loss  : {running_loss_val}, val acc  : {accuracy_val}\\n\"\n",
    "      )\n",
    "    \n",
    "    if ((epoch + 1) == num_epochs):\n",
    "      tqdm.write(\n",
    "        f\"      test loss : {running_loss_test}, test acc : {accuracy_test}\"\n",
    "      )\n",
    "      \n",
    "      training_logs[\"accuracies\"][str(epoch)][\n",
    "        \"accuracy_test\"\n",
    "      ] = accuracy_test.cpu().item()\n",
    "      \n",
    "      training_logs[\"losses\"][str(epoch)][\n",
    "        \"loss_test\"\n",
    "      ] = running_loss_test\n",
    "      \n",
    "    if (ckp_freq != None and (epoch + 1) % ckp_freq == 0):\n",
    "      \n",
    "      ckp_path = f\"{ckp_folder}/{train_id}\"\n",
    "      \n",
    "      store_ckp(\n",
    "        model=model, optimizer=optimizer, ckp_path=ckp_path, epoch=epoch, \n",
    "        train_id=train_id,\n",
    "        loss_train=running_loss_train, \n",
    "        loss_val=running_loss_val, \n",
    "        loss_test=running_loss_test\n",
    "      )\n",
    "  \n",
    "  training_end_time = time.time()\n",
    "\n",
    "  training_logs[\"training_time_secs\"] = training_end_time - training_start_time\n",
    "\n",
    "  return training_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design motivations\n",
    "\n",
    "First layers --> neural compression layers --> dimensionality reduction to roughly match dimensions of this paper https://arxiv.org/pdf/1703.01789.pdf\n",
    "\n",
    "Mid and final layers --> taken 1:1 from the paper linked above\n",
    "\n",
    "Batch norm placed BEFORE the activation function, as described in the og paper https://arxiv.org/abs/1502.03167 and explained by Bengio in his DL book https://www.deeplearningbook.org/contents/optimization.html in section 8.7.1\n",
    "\n",
    "Dropout placed according to the og paper: https://arxiv.org/pdf/1207.0580.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(\n",
    "    self, \n",
    "    neural_compression_num_layers, \n",
    "    neural_compression_kernel_sizes, neural_compression_strides, \n",
    "    neural_compression_in_channels, neural_compression_num_filters,\n",
    "    neural_compression_pool_sizes, neural_compression_pool_strides,\n",
    "    classification_num_layers, \n",
    "    classification_kernel_sizes, classification_strides, \n",
    "    classification_in_channels, classification_num_filters,\n",
    "    classification_pool_sizes, classification_pool_strides,\n",
    "    dropout_p\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.neural_compression_num_layers = neural_compression_num_layers \n",
    "    self.neural_compression_kernel_sizes = neural_compression_kernel_sizes \n",
    "    self.neural_compression_strides = neural_compression_strides \n",
    "    self.neural_compression_in_channels = neural_compression_in_channels \n",
    "    self.neural_compression_num_filters = neural_compression_num_filters\n",
    "    self.neural_compression_pool_sizes = neural_compression_pool_sizes \n",
    "    self.neural_compression_pool_strides = neural_compression_pool_strides\n",
    "    self.classification_num_layers = classification_num_layers \n",
    "    self.classification_kernel_sizes = classification_kernel_sizes \n",
    "    self.classification_strides = classification_strides \n",
    "    self.classification_in_channels = classification_in_channels \n",
    "    self.classification_num_filters = classification_num_filters\n",
    "    self.classification_pool_sizes = classification_pool_sizes \n",
    "    self.classification_pool_strides = classification_pool_strides\n",
    "    \n",
    "    self.bns = {\n",
    "      \"8\": nn.BatchNorm1d(num_features=8),\n",
    "      \"16\": nn.BatchNorm1d(num_features=16),\n",
    "      \"32\": nn.BatchNorm1d(num_features=32),\n",
    "      \"64\": nn.BatchNorm1d(num_features=64),\n",
    "      \"128\": nn.BatchNorm1d(num_features=128),\n",
    "      \"256\": nn.BatchNorm1d(num_features=256),\n",
    "      \"512\": nn.BatchNorm1d(num_features=512)\n",
    "    }\n",
    "    \n",
    "    self.dropout_p = dropout_p\n",
    "        \n",
    "    ### BEGIN neural compression layers. \n",
    "    ### See above cell for full explanation.\n",
    "    \n",
    "    in_channels = self.neural_compression_in_channels\n",
    "\n",
    "    self.neural_compression_block = nn.Sequential()\n",
    "    \n",
    "    for i in range(neural_compression_num_layers):\n",
    "      \n",
    "      neural_compression_conv_layer = nn.Conv1d(\n",
    "        kernel_size=self.neural_compression_kernel_sizes[i],\n",
    "        stride=self.neural_compression_strides[i],\n",
    "        in_channels=in_channels,\n",
    "        out_channels=self.neural_compression_num_filters[i]\n",
    "      )\n",
    "      \n",
    "      neural_compression_pool_layer = nn.MaxPool1d(\n",
    "        kernel_size=self.neural_compression_pool_sizes[i], \n",
    "        stride=self.neural_compression_pool_strides[i]\n",
    "      )\n",
    "\n",
    "      self.neural_compression_block.add_module(\n",
    "        name=f\"neural_compression_conv_{i}\",\n",
    "        module=neural_compression_conv_layer\n",
    "      )\n",
    "\n",
    "      self.neural_compression_block.add_module(\n",
    "        name=f\"neural_compression_pool_{i}\",\n",
    "        module=neural_compression_pool_layer\n",
    "      )\n",
    "\n",
    "      self.neural_compression_block.add_module(\n",
    "        name=f\"neural_compression_batchnorm_{i}\",\n",
    "        module=self.bns[str(self.neural_compression_num_filters[i])]\n",
    "      )\n",
    "\n",
    "      self.neural_compression_block.add_module(\n",
    "        name=f\"neural_compression_activation_{i}\",\n",
    "        module=nn.ReLU()\n",
    "      )\n",
    "      \n",
    "      in_channels = self.neural_compression_num_filters[i]\n",
    "\n",
    "      \n",
    "    # self.neural_compression_block = nn.Sequential(*neural_compression_layers)\n",
    "    \n",
    "    ### END Neural compression layers\n",
    "    \n",
    "    ### BEGIN classification layers. \n",
    "    ### See above cell for full explanation.\n",
    "    \n",
    "    in_channels = self.classification_in_channels\n",
    "\n",
    "    self.classification_block = nn.Sequential()\n",
    "    \n",
    "    for i in range(classification_num_layers):\n",
    "      classification_conv_layer = nn.Conv1d(\n",
    "        kernel_size=self.classification_kernel_sizes[i],\n",
    "        stride=self.classification_strides[i],\n",
    "        in_channels=in_channels,\n",
    "        out_channels=self.classification_num_filters[i]\n",
    "      )\n",
    "      \n",
    "      classification_pooling_layer = nn.MaxPool1d(\n",
    "        kernel_size=self.classification_pool_sizes[i],\n",
    "        stride=self.classification_pool_strides[i],\n",
    "      )\n",
    "      \n",
    "      in_channels = self.classification_num_filters[i]\n",
    "      \n",
    "      self.classification_block.add_module(\n",
    "        name=f\"classification_conv_{i}\", module=classification_conv_layer\n",
    "      )\n",
    "      \n",
    "      self.classification_block.add_module(\n",
    "        name=f\"classification_pool_{i}\", module=classification_pooling_layer\n",
    "      )\n",
    "      \n",
    "      if (i < classification_num_layers - 1):\n",
    "        \n",
    "        self.classification_block.add_module(\n",
    "          name=f\"classification_batchnorm_{i}\", \n",
    "          module=self.bns[str(self.classification_num_filters[i])]\n",
    "        )\n",
    "      \n",
    "      if (i < classification_num_layers - 1):\n",
    "        \n",
    "        self.classification_block.add_module(\n",
    "          name=f\"classification_activation_{i}\", module=nn.ReLU()\n",
    "        )\n",
    "\n",
    "      else:\n",
    "        \n",
    "        self.classification_block.add_module(\n",
    "          name=f\"classification_activation_{i}\", module=nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "      if (i == classification_num_layers - 2):\n",
    "\n",
    "        self.classification_block.add_module(\n",
    "          name=f\"classification_dropout_{i}\", \n",
    "          module=nn.Dropout(p=self.dropout_p)\n",
    "        )\n",
    "    \n",
    "    ### END classification layers. \n",
    "    ### See above cell for full explanation.\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.neural_compression_block(x)\n",
    "    \n",
    "    x = self.classification_block(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "  def get_model_setup(self):\n",
    "    \n",
    "    return {\n",
    "      \"neural_compression_num_layers\": self.neural_compression_num_layers, \n",
    "      \"neural_compression_kernel_sizes\": self.neural_compression_kernel_sizes, \n",
    "      \"neural_compression_strides\": self.neural_compression_strides, \n",
    "      \"neural_compression_in_channels\": self.neural_compression_in_channels, \n",
    "      \"neural_compression_num_filters\": self.neural_compression_num_filters,\n",
    "      \"neural_compression_pool_sizes\": self.neural_compression_pool_sizes, \n",
    "      \"neural_compression_pool_strides\": self.neural_compression_pool_strides,\n",
    "      \"classification_num_layers\": self.classification_num_layers, \n",
    "      \"classification_kernel_sizes\": self.classification_kernel_sizes, \n",
    "      \"classification_strides\": self.classification_strides, \n",
    "      \"classification_in_channels\": self.classification_in_channels, \n",
    "      \"classification_num_filters\": self.classification_num_filters,\n",
    "      \"classification_pool_sizes\": self.classification_pool_sizes, \n",
    "      \"classification_pool_strides\": self.classification_pool_strides,\n",
    "      \"dropout_p\": self.dropout_p\n",
    "    }\n",
    "\n",
    "  def reset_weights(m):\n",
    "    \n",
    "    for layer in m.children():\n",
    "      if hasattr(layer, 'reset_parameters'):\n",
    "        print(f'Resetting trainable parameters of layer = {layer}')\n",
    "        layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(stats):\n",
    "  epochs = stats[\"training_logs\"][\"losses\"].keys()\n",
    "  \n",
    "  loss_train = [\n",
    "    j[\"loss_train\"] for j in stats[\"training_logs\"][\"losses\"].values()\n",
    "  ]\n",
    "  \n",
    "  loss_val = [j[\"loss_val\"] for j in stats[\"training_logs\"][\"losses\"].values()]\n",
    "\n",
    "  sns.lineplot(\n",
    "    x=epochs,\n",
    "    y=loss_train,\n",
    "    legend=\"full\",\n",
    "    label=\"train loss\"\n",
    "  )\n",
    "\n",
    "  sns.lineplot(\n",
    "    x=epochs,\n",
    "    y=loss_val,\n",
    "    legend=\"full\",\n",
    "    label=\"val loss\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross validation for hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_compression_num_layers   = 3\n",
    "neural_compression_kernel_sizes = [ 3,  3,  3]\n",
    "neural_compression_strides      = [ 3,  2,  2]\n",
    "neural_compression_num_filters  = [ 8, 16, 32]\n",
    "neural_compression_in_channels  = 1\n",
    "neural_compression_pool_sizes   = [3, 3, 3]\n",
    "neural_compression_pool_strides = [1, 1, 2]\n",
    "\n",
    "classification_num_layers    = 4\n",
    "classification_kernel_sizes  = [ 3,  3,  3, 3]\n",
    "classification_strides       = [ 3,  3,  3, 3]\n",
    "classification_in_channels   = neural_compression_num_filters[-1]\n",
    "classification_num_filters   = [ 32, 64, 128, 6]\n",
    "classification_pool_sizes    = [3, 3, 3, 3]\n",
    "classification_pool_strides  = [3, 3, 3, 3]\n",
    "\n",
    "DROPOUT_P = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_factory():\n",
    "  return CNN(\n",
    "  # neural compression layers parameters\n",
    "  neural_compression_num_layers=neural_compression_num_layers,\n",
    "  neural_compression_kernel_sizes=neural_compression_kernel_sizes, \n",
    "  neural_compression_strides=neural_compression_strides, \n",
    "  neural_compression_in_channels=neural_compression_in_channels, \n",
    "  neural_compression_num_filters=neural_compression_num_filters,\n",
    "  neural_compression_pool_sizes=neural_compression_pool_sizes,\n",
    "  neural_compression_pool_strides=neural_compression_pool_strides,\n",
    "  # classification layers parameters\n",
    "  classification_num_layers=classification_num_layers,\n",
    "  classification_kernel_sizes=classification_kernel_sizes, \n",
    "  classification_strides=classification_strides, \n",
    "  classification_in_channels=classification_in_channels, \n",
    "  classification_num_filters=classification_num_filters,\n",
    "  classification_pool_sizes=classification_pool_sizes,\n",
    "  classification_pool_strides=classification_pool_strides,\n",
    "  dropout_p=DROPOUT_P\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-6\n",
    "OPTIMIZER_NAME = \"SGD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_factory(optimizer_name, model, lr, momentum, weight_decay):\n",
    "\n",
    "  if optimizer_name == \"SGD\":\n",
    "    optimizer = optim.SGD(\n",
    "      model.parameters(), \n",
    "      lr=lr, \n",
    "      momentum=momentum,\n",
    "      nesterov=True,\n",
    "      weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "  elif optimizer_name == \"Adam\":\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "      model.parameters(),\n",
    "      lr=lr,\n",
    "      weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "\n",
    "  optimizer_parameters = {\n",
    "    \"optimizer\": optimizer,\n",
    "    \"lr\": lr, \n",
    "    \"momentum\": momentum, \n",
    "    \"weight_decay\": weight_decay\n",
    "  }  \n",
    "\n",
    "  return optimizer, optimizer_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_FOLD_CV_NUM_FOLDS = 2\n",
    "\n",
    "K_FOLD_CV_BATCH_SIZE = 16\n",
    "\n",
    "K_FOLD_CV_NUM_EPOCHS = 2\n",
    "\n",
    "K_FOLD_CV_PRINT_FREQ = 1000\n",
    "\n",
    "K_FOLD_CV_CKP_FREQ = 1000\n",
    "\n",
    "K_FOLD_CV_CKP_FOLDER = TRAINING_LOGS_FOLDER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_models = [cnn_factory() for _ in range(0, K_FOLD_CV_NUM_FOLDS)]\n",
    "\n",
    "cv_criterions = [nn.CrossEntropyLoss() for _ in range(0, K_FOLD_CV_NUM_FOLDS)]\n",
    "\n",
    "cv_opts = [\n",
    "  optimizer_factory(\n",
    "    optimizer_name=OPTIMIZER_NAME,\n",
    "    model=cv_models[i],\n",
    "    lr=LR,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    "  ) for i in range(0, K_FOLD_CV_NUM_FOLDS)\n",
    "]\n",
    "\n",
    "cv_optimizers = [opt for opt, _ in cv_opts]\n",
    "cv_optimizers_params = [opt_params for _, opt_params in cv_opts]\n",
    "\n",
    "cv_train_dls = []\n",
    "cv_val_dls = []\n",
    "cv_test_dls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=K_FOLD_CV_NUM_FOLDS, shuffle=True)\n",
    "\n",
    "cv_dataset = torch.utils.data.ConcatDataset(\n",
    "    [fma_dataset_train, fma_dataset_val]\n",
    ")\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(k_fold.split(cv_dataset)):\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    cv_train_dls.append(\n",
    "        torch.utils.data.DataLoader(\n",
    "            cv_dataset, batch_size=K_FOLD_CV_BATCH_SIZE, sampler=train_subsampler\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cv_val_dls.append(\n",
    "        torch.utils.data.DataLoader(\n",
    "            cv_dataset, batch_size=K_FOLD_CV_BATCH_SIZE, sampler=test_subsampler\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cv_test_dls.append(\n",
    "        torch.utils.data.DataLoader(\n",
    "            fma_dataset_test, batch_size=K_FOLD_CV_BATCH_SIZE\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_k_fold_cv(\n",
    "  cv_num_folds,\n",
    "  cv_models, cv_optimizers, cv_criterions,\n",
    "  batch_size, \n",
    "  cv_train_dls, cv_val_dls, cv_test_dls, \n",
    "  cv_num_epochs, \n",
    "  cv_device, \n",
    "  cv_print_freq, cv_ckp_freq, \n",
    "  cv_ckp_folder\n",
    "):\n",
    "\n",
    "  pbar_folds = tqdm(range(cv_num_folds), colour=\"#b22222\")\n",
    "\n",
    "  for fold in pbar_folds:\n",
    "    pbar_folds.set_description(f\"fold {fold + 1}\")\n",
    "\n",
    "    train_model(\n",
    "      model=cv_models[fold], \n",
    "      optimizer=cv_optimizers[fold], criterion=cv_criterions[fold],\n",
    "      batch_size=batch_size,\n",
    "      train_dl=cv_train_dls[fold], val_dl=cv_val_dls[fold], test_dl=cv_test_dls[fold],\n",
    "      num_epochs=cv_num_epochs, \n",
    "      device=cv_device,\n",
    "      print_freq=cv_print_freq, ckp_freq=cv_ckp_freq, \n",
    "      ckp_folder=cv_ckp_folder \n",
    "    )\n",
    "\n",
    "    pbar_folds.update(1)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10b819d79bc4c5b93110bd18128a135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa37dcee721498fb6061769e0538575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9326691e580a490c93706c2b5368770d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15aa52b5e3a4ce7b3189b13c6a076aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c064b4b1124adba106224b26bacaa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      test loss : 16.88296341896057, test acc : 0.20000000298023224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da47381801148a0a01715577b8ce912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0f569c1f5e41dfb4aa3b05a0096184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf18ec644ee24afca9404afc47e76a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec71385ec774e55aea8a94303233a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      test loss : 16.93841004371643, test acc : 0.20000000298023224\n"
     ]
    }
   ],
   "source": [
    "perform_k_fold_cv(\n",
    "  cv_num_folds=K_FOLD_CV_NUM_FOLDS,\n",
    "  cv_models=cv_models, cv_optimizers=cv_optimizers, cv_criterions=cv_criterions,\n",
    "  batch_size=K_FOLD_CV_BATCH_SIZE, \n",
    "  cv_train_dls=cv_train_dls, cv_val_dls=cv_val_dls, cv_test_dls=cv_test_dls,\n",
    "  cv_num_epochs=K_FOLD_CV_NUM_EPOCHS, \n",
    "  cv_device=device, \n",
    "  cv_print_freq=K_FOLD_CV_PRINT_FREQ, cv_ckp_freq=K_FOLD_CV_CKP_FREQ, \n",
    "  cv_ckp_folder=K_FOLD_CV_CKP_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "PRINT_FREQ = 1\n",
    "CKP_FREQ = 3\n",
    "\n",
    "PERFORM_TRAINING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f34c43a7094568ac510a0a9e031571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88525974953b4b5fab1fda3d6753f323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9bfb9314ae4fe28c9e6ccc8551f7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "      train loss: 111.61591482162476, train acc: 0.08064515888690948\n",
      "      val loss  : 32.27314043045044, val acc  : 0.2222222238779068\n",
      "\n",
      "epoch: 2\n",
      "      train loss: 110.97382140159607, train acc: 0.25806450843811035\n",
      "      val loss  : 32.27778601646423, val acc  : 0.2222222238779068\n",
      "\n",
      "epoch: 3\n",
      "      train loss: 111.12460327148438, train acc: 0.11290322244167328\n",
      "      val loss  : 32.2803635597229, val acc  : 0.2777777910232544\n",
      "\n",
      "epoch: 4\n",
      "      train loss: 110.15311193466187, train acc: 0.19354838132858276\n",
      "      val loss  : 32.228341579437256, val acc  : 0.2777777910232544\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4006b9e92df44c8384822159f64e0116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "      train loss: 109.1954038143158, train acc: 0.24193547666072845\n",
      "      val loss  : 32.1153666973114, val acc  : 0.2777777910232544\n",
      "\n",
      "      test loss : 16.773488759994507, test acc : 0.5\n"
     ]
    }
   ],
   "source": [
    "if PERFORM_TRAINING:\n",
    "  \n",
    "  training_logs = train_model(\n",
    "    model=cnn_attempt_1, optimizer=optimizer, criterion=criterion,\n",
    "    batch_size=BATCH_SIZE, train_dl=fma_dataloader_train, \n",
    "    val_dl=fma_dataloader_val,test_dl=fma_dataloader_test,\n",
    "    num_epochs=NUM_EPOCHS, device=device,\n",
    "    print_freq=PRINT_FREQ,\n",
    "    ckp_folder=TRAINING_LOGS_FOLDER, ckp_freq=CKP_FREQ\n",
    "  )\n",
    "\n",
    "  stats = {\n",
    "    \"data_logs\": data_logs,\n",
    "    \"optimizer_parameters\": optimizer_parameters,\n",
    "    \"model_setup\": cnn_attempt_1.get_model_setup(),\n",
    "    \"training_logs\": training_logs,\n",
    "  }\n",
    "\n",
    "  train_id = training_logs[\"train_id\"]\n",
    "  save_dict_to_disk(\n",
    "    dict=stats,\n",
    "    full_path=f\"{TRAINING_LOGS_FOLDER}/{train_id}/{train_id}.json\"\n",
    "  )\n",
    "\n",
    "else:\n",
    "\n",
    "  train_id = \"05_12_2022_11_52_21\"\n",
    "\n",
    "  EPOCH_TO_LOAD = 3\n",
    "  CKP_PATH = f\"./logs/{train_id}/{train_id}_epoch_{EPOCH_TO_LOAD}\"\n",
    "  PERFORM_LOADING_SANITY_CHECK = True\n",
    "\n",
    "  cnn_attempt_1 = load_ckp(\n",
    "    ckp_path=CKP_PATH,\n",
    "    perform_loading_sanity_check=PERFORM_LOADING_SANITY_CHECK\n",
    "  )\n",
    "\n",
    "  STATS_PATH = f\"./logs/{train_id}/{train_id}.json\"\n",
    "\n",
    "  stats_json = open(STATS_PATH)\n",
    "\n",
    "  stats = json.load(stats_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curves(stats=stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
