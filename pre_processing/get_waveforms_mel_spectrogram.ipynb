{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import gc\n",
    "\n",
    "MANUAL_SEED = 69\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "  \n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "import copy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  axs.set_title(title or 'Spectrogram (db)')\n",
    "  axs.set_ylabel(ylabel)\n",
    "  axs.set_xlabel('frame')\n",
    "  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
    "  if xmax:\n",
    "    axs.set_xlim((0, xmax))\n",
    "  fig.colorbar(im, ax=axs)\n",
    "  plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir_if_absent(dir_path):\n",
    "  \n",
    "  # print(\"making dir: \", dir_path)\n",
    "  \n",
    "  if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMADataset(Dataset):\n",
    "\n",
    "  def __init__(\n",
    "    self, path, normalize_audio, audio_num_frames, waveform_mean, waveform_std,\n",
    "    mel_spectrogram_n_fft, mel_spectrogram_win_length, \n",
    "    mel_spectrogram_hop_length, mel_spectrogram_n_mels\n",
    "  ):\n",
    "    self.path = path\n",
    "    self.normalize_audio = normalize_audio\n",
    "    self.audio_num_frames = audio_num_frames\n",
    "    self.waveform_mean = waveform_mean\n",
    "    self.waveform_std = waveform_std\n",
    "\n",
    "    self.mel_spectrogram_n_fft = mel_spectrogram_n_fft\n",
    "    self.mel_spectrogram_win_length = mel_spectrogram_win_length\n",
    "    self.mel_spectrogram_hop_length = mel_spectrogram_hop_length\n",
    "    self.mel_spectrogram_n_mels = mel_spectrogram_n_mels\n",
    "    \n",
    "    self.data = self._load_audio_list()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \n",
    "    try: \n",
    "      \n",
    "      waveform, sample_rate = torchaudio.load(\n",
    "        filepath=self.data[idx], \n",
    "        normalize=self.normalize_audio,\n",
    "        num_frames=self.audio_num_frames\n",
    "      )\n",
    "      \n",
    "      if waveform.shape[1] < self.audio_num_frames:\n",
    "        waveform = self._apply_padding(waveform)\n",
    "      \n",
    "      label = self.data[idx].split(\"/\")[-2]\n",
    "      label_one_hot = self._label_from_str_to_one_hot(label)\n",
    "\n",
    "      mel_spectrogram = self._get_mel_spectrogram(\n",
    "        sample_rate=sample_rate, waveform=waveform\n",
    "      )\n",
    "\n",
    "      mel_spectrogram_export_full_path = self.data[idx]\n",
    "      mel_spectrogram_export_full_path = mel_spectrogram_export_full_path.replace(\n",
    "        \"audio\", \"mel_spectrogram\"\n",
    "      )\n",
    "\n",
    "      og_dataset_name = mel_spectrogram_export_full_path.split(\"/\")[3]\n",
    "      mel_spec_dataset_name = f\"{og_dataset_name}_n_fft_{self.mel_spectrogram_n_fft}_win_length_{self.mel_spectrogram_win_length}_hop_length_{self.mel_spectrogram_hop_length}_n_mels_{self.mel_spectrogram_n_mels}\"\n",
    "\n",
    "\n",
    "      mel_spectrogram_export_full_path_as_list = mel_spectrogram_export_full_path.split(\"/\")\n",
    "      mel_spectrogram_export_full_path_as_list[3] = mel_spec_dataset_name\n",
    "\n",
    "      mel_spectrogram_export_full_path = \"/\".join(\n",
    "        mel_spectrogram_export_full_path_as_list\n",
    "      )\n",
    "\n",
    "      mel_spectrogram_export_dir = \"/\".join(\n",
    "        mel_spectrogram_export_full_path.split(\"/\")[:-1]\n",
    "      )\n",
    "\n",
    "      make_dir_if_absent(mel_spectrogram_export_dir)\n",
    "\n",
    "      mel_spectrogram_export_full_path = mel_spectrogram_export_full_path.replace(\"mp3\", \"mel_spec\")\n",
    "      \n",
    "      torch.save(\n",
    "        mel_spectrogram, mel_spectrogram_export_full_path\n",
    "      )\n",
    "\n",
    "      waveform_path = self.data[idx]\n",
    "\n",
    "      waveform_path = waveform_path.replace(\"audio\", \"waveform\")\n",
    "      \n",
    "      waveform_path = waveform_path.replace(\".mp3\", \".waveform\")\n",
    "\n",
    "      waveform_export_dir = \"/\".join(\n",
    "        waveform_path.split(\"/\")[:-1]\n",
    "      )\n",
    "\n",
    "      make_dir_if_absent(waveform_export_dir)\n",
    "\n",
    "      torch.save(waveform, waveform_path)\n",
    "\n",
    "      \n",
    "      return waveform, waveform_path, mel_spectrogram, mel_spectrogram_export_full_path, label_one_hot\n",
    "    \n",
    "    except Exception as e:\n",
    "      print(f\"Got the following exception for the file {self.data[idx]}\")\n",
    "      print(\"\\n\\n\")\n",
    "      print(e)\n",
    "\n",
    "\n",
    "  def _get_mel_spectrogram(self, sample_rate, waveform):\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=self.mel_spectrogram_n_fft,\n",
    "        win_length=self.mel_spectrogram_win_length,\n",
    "        hop_length=self.mel_spectrogram_hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm='slaney',\n",
    "        onesided=True,\n",
    "        n_mels=self.mel_spectrogram_n_mels,\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "\n",
    "    return mel_spectrogram(waveform)\n",
    "      \n",
    "  def _apply_padding(self, to_pad):\n",
    "    padding_size = self.audio_num_frames - to_pad.shape[1]\n",
    "    \n",
    "    return torch.nn.functional.pad(\n",
    "      to_pad, (0, padding_size)\n",
    "    )\n",
    "  \n",
    "  def _label_from_str_to_one_hot(self, label_str: str): \n",
    "  \n",
    "    if label_str == \"Pop\":\n",
    "      return torch.tensor([1, 0, 0, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Hip-Hop\":\n",
    "      return torch.tensor([0, 1, 0, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Electronic\":\n",
    "      return torch.tensor([0, 0, 1, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Rock\":\n",
    "      return torch.tensor([0, 0, 0, 1, 0, 0]).float()\n",
    "\n",
    "    if label_str == \"Folk\":\n",
    "      return torch.tensor([0, 0, 0, 0, 1, 0]).float()\n",
    "\n",
    "    if label_str == \"Jazz\":\n",
    "      return torch.tensor([0, 0, 0, 0, 0, 1]).float()\n",
    "    \n",
    "  \n",
    "  def _load_audio_list(self):\n",
    "    \n",
    "    audio_path_list = []\n",
    "    \n",
    "    for path, subdirs, files in tqdm(os.walk(self.path), colour=\"magenta\"):\n",
    "      for name in files:\n",
    "          \n",
    "        file_audio_path = os.path.join(path, name)\n",
    "        \n",
    "        audio_path_list.append(file_audio_path)\n",
    "        \n",
    "    return audio_path_list\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = \"s\"\n",
    "DATASET_FOLDER = \"../data/audio\"\n",
    "\n",
    "DATASET_NUM_SAMPLES_PER_SECOND = 22050\n",
    "DATASET_NUM_CHANNELS = 1\n",
    "\n",
    "DATASET_NAME = f\"fma_{DATASET_SIZE}_resampled_{DATASET_NUM_SAMPLES_PER_SECOND}_rechanneled_{DATASET_NUM_CHANNELS}\"\n",
    "\n",
    "dataset_path = f\"{DATASET_FOLDER}/{DATASET_NAME}\"\n",
    "\n",
    "TRAINING_LOGS_FOLDER = \"./logs\"\n",
    "\n",
    "NORMALIZE_AUDIO = True\n",
    "AUDIO_NUM_FRAMES_DICT = {\n",
    "  (8000, 1) : 238000,\n",
    "  (22050, 1): 660000\n",
    "}\n",
    "AUDIO_NUM_FRAMES = AUDIO_NUM_FRAMES_DICT[\n",
    "  (DATASET_NUM_SAMPLES_PER_SECOND, DATASET_NUM_CHANNELS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1966fbcef84bfc8b849fb470ecd5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fma_dataset = FMADataset(\n",
    "  path=dataset_path, \n",
    "  normalize_audio=NORMALIZE_AUDIO, \n",
    "  audio_num_frames=AUDIO_NUM_FRAMES,\n",
    "  waveform_mean=-1,  \n",
    "  waveform_std=-1, \n",
    "  mel_spectrogram_n_fft=1024, \n",
    "  mel_spectrogram_win_length=None, \n",
    "  mel_spectrogram_hop_length=128,\n",
    "  mel_spectrogram_n_mels=128\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_waveforms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b8312abb5c414186ce4ce2dac4ed09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data_sample in tqdm(fma_dataset, colour=\"cyan\"):\n",
    "  waveform, waveform_path, mel_spectrogram, _, _ = data_sample\n",
    "  list_waveforms.append(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_waveform = torch.load(\n",
    "  waveform_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 660000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_waveform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(loaded_waveform, waveform).sum()/torch.isclose(loaded_waveform, waveform).numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_waveforms = torch.cat(list_waveforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
