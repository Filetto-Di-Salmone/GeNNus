{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import gc\n",
    "\n",
    "MANUAL_SEED = 69\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "  \n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "import copy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  axs.set_title(title or 'Spectrogram (db)')\n",
    "  axs.set_ylabel(ylabel)\n",
    "  axs.set_xlabel('frame')\n",
    "  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
    "  if xmax:\n",
    "    axs.set_xlim((0, xmax))\n",
    "  fig.colorbar(im, ax=axs)\n",
    "  plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir_if_absent(dir_path):\n",
    "  \n",
    "  # print(\"making dir: \", dir_path)\n",
    "  \n",
    "  if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMADataset(Dataset):\n",
    "\n",
    "  def __init__(\n",
    "    self, path, normalize_audio, audio_num_frames, waveform_mean, waveform_std,\n",
    "    mel_spectrogram_n_fft, mel_spectrogram_win_length, \n",
    "    mel_spectrogram_hop_length, mel_spectrogram_n_mels\n",
    "  ):\n",
    "    self.path = path\n",
    "    self.normalize_audio = normalize_audio\n",
    "    self.audio_num_frames = audio_num_frames\n",
    "    self.waveform_mean = waveform_mean\n",
    "    self.waveform_std = waveform_std\n",
    "\n",
    "    self.mel_spectrogram_n_fft = mel_spectrogram_n_fft\n",
    "    self.mel_spectrogram_win_length = mel_spectrogram_win_length\n",
    "    self.mel_spectrogram_hop_length = mel_spectrogram_hop_length\n",
    "    self.mel_spectrogram_n_mels = mel_spectrogram_n_mels\n",
    "    \n",
    "    self.data = self._load_audio_list()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \n",
    "    try: \n",
    "      \n",
    "      waveform, sample_rate = torchaudio.load(\n",
    "        filepath=self.data[idx], \n",
    "        normalize=self.normalize_audio,\n",
    "        num_frames=self.audio_num_frames\n",
    "      )\n",
    "      \n",
    "      if waveform.shape[1] < self.audio_num_frames:\n",
    "        waveform = self._apply_padding(waveform)\n",
    "      \n",
    "      label = self.data[idx].split(\"/\")[-2]\n",
    "      label_one_hot = self._label_from_str_to_one_hot(label)\n",
    "\n",
    "      mel_spectrogram = self._get_mel_spectrogram(\n",
    "        sample_rate=sample_rate, waveform=waveform\n",
    "      )\n",
    "\n",
    "      mel_spectrogram_export_full_path = self.data[idx]\n",
    "      mel_spectrogram_export_full_path = mel_spectrogram_export_full_path.replace(\n",
    "        \"audio\", \"mel_spectrogram\"\n",
    "      )\n",
    "\n",
    "      og_dataset_name = mel_spectrogram_export_full_path.split(\"/\")[3]\n",
    "      mel_spec_dataset_name = f\"{og_dataset_name}_n_fft_{self.mel_spectrogram_n_fft}_win_length_{self.mel_spectrogram_win_length}_hop_length_{self.mel_spectrogram_hop_length}_n_mels_{self.mel_spectrogram_n_mels}\"\n",
    "\n",
    "\n",
    "      mel_spectrogram_export_full_path_as_list = mel_spectrogram_export_full_path.split(\"/\")\n",
    "      mel_spectrogram_export_full_path_as_list[3] = mel_spec_dataset_name\n",
    "\n",
    "      mel_spectrogram_export_full_path = \"/\".join(\n",
    "        mel_spectrogram_export_full_path_as_list\n",
    "      )\n",
    "\n",
    "      mel_spectrogram_export_dir = \"/\".join(\n",
    "        mel_spectrogram_export_full_path.split(\"/\")[:-1]\n",
    "      )\n",
    "\n",
    "      make_dir_if_absent(mel_spectrogram_export_dir)\n",
    "\n",
    "      mel_spectrogram_export_full_path = mel_spectrogram_export_full_path.replace(\"mp3\", \"mel_spec\")\n",
    "      \n",
    "      torch.save(\n",
    "        mel_spectrogram, mel_spectrogram_export_full_path\n",
    "      )\n",
    "\n",
    "      waveform_path = self.data[idx]\n",
    "\n",
    "      waveform_path = waveform_path.replace(\"audio\", \"waveform\")\n",
    "      \n",
    "      waveform_path = waveform_path.replace(\".mp3\", \".waveform\")\n",
    "\n",
    "      waveform_export_dir = \"/\".join(\n",
    "        waveform_path.split(\"/\")[:-1]\n",
    "      )\n",
    "\n",
    "      make_dir_if_absent(waveform_export_dir)\n",
    "\n",
    "      torch.save(waveform, waveform_path)\n",
    "\n",
    "      \n",
    "      return waveform, waveform_path, mel_spectrogram, mel_spectrogram_export_full_path, label_one_hot\n",
    "    \n",
    "    except Exception as e:\n",
    "      print(f\"Got the following exception for the file {self.data[idx]}\")\n",
    "      print(\"\\n\\n\")\n",
    "      print(e)\n",
    "\n",
    "\n",
    "  def _get_mel_spectrogram(self, sample_rate, waveform):\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=self.mel_spectrogram_n_fft,\n",
    "        win_length=self.mel_spectrogram_win_length,\n",
    "        hop_length=self.mel_spectrogram_hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm='slaney',\n",
    "        onesided=True,\n",
    "        n_mels=self.mel_spectrogram_n_mels,\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "\n",
    "    return mel_spectrogram(waveform)\n",
    "      \n",
    "  def _apply_padding(self, to_pad):\n",
    "    padding_size = self.audio_num_frames - to_pad.shape[1]\n",
    "    \n",
    "    return torch.nn.functional.pad(\n",
    "      to_pad, (0, padding_size)\n",
    "    )\n",
    "  \n",
    "  def _label_from_str_to_one_hot(self, label_str: str): \n",
    "  \n",
    "    if label_str == \"Pop\":\n",
    "      return torch.tensor([1, 0, 0, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Hip-Hop\":\n",
    "      return torch.tensor([0, 1, 0, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Electronic\":\n",
    "      return torch.tensor([0, 0, 1, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Rock\":\n",
    "      return torch.tensor([0, 0, 0, 1, 0, 0]).float()\n",
    "\n",
    "    if label_str == \"Folk\":\n",
    "      return torch.tensor([0, 0, 0, 0, 1, 0]).float()\n",
    "\n",
    "    if label_str == \"Jazz\":\n",
    "      return torch.tensor([0, 0, 0, 0, 0, 1]).float()\n",
    "    \n",
    "  \n",
    "  def _load_audio_list(self):\n",
    "    \n",
    "    audio_path_list = []\n",
    "    \n",
    "    for path, subdirs, files in tqdm(os.walk(self.path), colour=\"magenta\"):\n",
    "      for name in files:\n",
    "          \n",
    "        file_audio_path = os.path.join(path, name)\n",
    "        \n",
    "        audio_path_list.append(file_audio_path)\n",
    "        \n",
    "    return audio_path_list\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = \"xl\"\n",
    "DATASET_FOLDER = \"./data/audio\"\n",
    "\n",
    "DATASET_NUM_SAMPLES_PER_SECOND = 8000\n",
    "DATASET_NUM_CHANNELS = 1\n",
    "\n",
    "DATASET_NAME = f\"fma_{DATASET_SIZE}_resampled_{DATASET_NUM_SAMPLES_PER_SECOND}_rechanneled_{DATASET_NUM_CHANNELS}\"\n",
    "\n",
    "dataset_path = f\"{DATASET_FOLDER}/{DATASET_NAME}\"\n",
    "\n",
    "TRAINING_LOGS_FOLDER = \"./logs\"\n",
    "\n",
    "NORMALIZE_AUDIO = True\n",
    "AUDIO_NUM_FRAMES = 238000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_statistics_json = open(\n",
    "#   f\"{dataset_path}_summary_statistics/{DATASET_NAME}_summary_statistics.json\"\n",
    "# )\n",
    "\n",
    "# summary_statistics_dict = json.load(summary_statistics_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae68c6fdaff4184bb218e2962bbb0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fma_dataset = FMADataset(\n",
    "  path=dataset_path, \n",
    "  normalize_audio=NORMALIZE_AUDIO, \n",
    "  audio_num_frames=AUDIO_NUM_FRAMES,\n",
    "  waveform_mean=-1,  \n",
    "  waveform_std=-1, \n",
    "  mel_spectrogram_n_fft=1024, \n",
    "  mel_spectrogram_win_length=None, \n",
    "  mel_spectrogram_hop_length=128,\n",
    "  mel_spectrogram_n_mels=128\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98191bb7c0824bd79c40007006b14436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for data_sample in tqdm(fma_dataset, colour=\"cyan\"):\n",
    "  waveform, waveform_path, _, _, _ = data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_waveform = torch.load(\n",
    "  waveform_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_waveform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.isclose(loaded_waveform, waveform).sum()/torch.isclose(loaded_waveform, waveform).numel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
