{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Data pre-processing </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_from_str_to_one_hot(label_str: str): \n",
    "  \n",
    "  if label_str == \"Pop\":\n",
    "    return torch.tensor([1, 0, 0, 0, 0, 0])\n",
    "  \n",
    "  if label_str == \"Hip-Hop\":\n",
    "    return torch.tensor([0, 1, 0, 0, 0, 0])\n",
    "  \n",
    "  if label_str == \"Electronic\":\n",
    "    return torch.tensor([0, 0, 1, 0, 0, 0])\n",
    "  \n",
    "  if label_str == \"Rock\":\n",
    "    return torch.tensor([0, 0, 0, 1, 0, 0])\n",
    "\n",
    "  if label_str == \"Folk\":\n",
    "    return torch.tensor([0, 0, 0, 0, 1, 0])\n",
    "\n",
    "  if label_str == \"Jazz\":\n",
    "    return torch.tensor([0, 0, 0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_audio_data(\n",
    "  dataset_path, normalize_audio: bool, audio_num_frames: int\n",
    "):\n",
    "  \n",
    "  audio_tensor_list = []\n",
    "  \n",
    "  num_audio_files_unable_to_open = 0\n",
    "\n",
    "  # TODO use glob instead of this\n",
    "  for path, subdirs, files in os.walk(dataset_path):\n",
    "    for name in tqdm(files, colour=\"red\"):\n",
    "        \n",
    "      file_audio_path = os.path.join(path, name)\n",
    "      \n",
    "      try:\n",
    "        waveform, sample_rate = torchaudio.load(\n",
    "          file_audio_path, normalize=normalize_audio,\n",
    "          num_frames=audio_num_frames\n",
    "        )\n",
    "        \n",
    "        label = file_audio_path.split(\"/\")[-2]\n",
    "        label_one_hot = label_from_str_to_one_hot(label)\n",
    "        \n",
    "        audio_tensor_list.append(\n",
    "          {\n",
    "            \"waveform\": waveform, \n",
    "            \"og_sample_rate\": sample_rate,\n",
    "            \"label_one_hot\": label_one_hot,\n",
    "            \"label\": label,\n",
    "            \"path\": file_audio_path,\n",
    "            \"hop_length\": -1\n",
    "          }\n",
    "        )\n",
    "\n",
    "        \n",
    "      except:\n",
    "        print(f\"[load_audio_data] error while loading {file_audio_path}\")\n",
    "        num_audio_files_unable_to_open += 1\n",
    "        continue\n",
    "  \n",
    "  return pd.DataFrame(audio_tensor_list)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = \"large\"\n",
    "SAMPLE_FREQ = 8000\n",
    "NUM_CHANNELS = 1\n",
    "HOP_LENGTH = 16\n",
    "\n",
    "DATASET_NAME = f\"fma_{DATASET_SIZE}_resampled_{SAMPLE_FREQ}_rechanneled_{NUM_CHANNELS}_hopped_{HOP_LENGTH}\"\n",
    "\n",
    "DATASET_FOLDER = \"../data/audio\"\n",
    "\n",
    "dataset_path = f\"{DATASET_FOLDER}/{DATASET_NAME}\"\n",
    "\n",
    "AUDIO_NUM_FRAMES = 238000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_audio_pd = load_raw_audio_data(\n",
    "  dataset_path=dataset_path, \n",
    "  normalize_audio=True, \n",
    "  audio_num_frames=AUDIO_NUM_FRAMES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 238000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_audio_pd.iloc[0][\"waveform\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"audio_hop\" --> take one sample ever hop_length elements\n",
    "def resample_audio(og_audio_pd, hop_length: int):\n",
    "  \n",
    "  resampled_audio_tensor_list = []\n",
    "  \n",
    "  for row_index, audio in tqdm(og_audio_pd.iterrows(), colour=\"green\"):\n",
    "    \n",
    "    resampled_waveform = torchaudio.functional.resample(\n",
    "        audio[\"waveform\"], \n",
    "        # as per Torch Audio docs, this is the way of performing \"hopping\" in a \n",
    "        # similar way as the Mel Spectrogram transform does\n",
    "        orig_freq=hop_length, new_freq=1\n",
    "      )\n",
    "    \n",
    "    resampled_audio_tensor_list.append(\n",
    "      {\n",
    "        \"waveform\": resampled_waveform, \n",
    "        \"og_sample_rate\": audio[\"og_sample_rate\"],\n",
    "        \"hop_length\": hop_length,\n",
    "        \"label_one_hot\": audio[\"label_one_hot\"].numpy(),\n",
    "        \"label\": audio[\"label\"],\n",
    "        \"path\": audio[\"path\"]\n",
    "      }\n",
    "    )\n",
    "    \n",
    "  return pd.DataFrame(resampled_audio_tensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:00, 419.42it/s]\n"
     ]
    }
   ],
   "source": [
    "hop_length = 16\n",
    "\n",
    "hopped_audio_pd = resample_audio(og_audio_pd, hop_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14875])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hopped_audio_pd.iloc[0][\"waveform\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from: https://stackoverflow.com/a/47626762\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "  \n",
    "  def default(self, obj):\n",
    "    \n",
    "    if isinstance(obj, np.ndarray):\n",
    "      return obj.tolist()\n",
    "    \n",
    "    return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def export_audio_data(audio_pd, audio_data_type):\n",
    "  \n",
    "  for row_ind, audio in tqdm(audio_pd.iterrows(), colour=\"yellow\"):\n",
    "    torchaudio.save(\n",
    "      audio[\"path\"], audio[audio_data_type], audio[\"og_sample_rate\"]\n",
    "  )\n",
    "    \n",
    "def export_mel_spectrogram(audio_pd, path, file_name):\n",
    "  \n",
    "  audio_pd.to_json(f\"{path}/{file_name}\")\n",
    "    \n",
    "    \n",
    "def export_audio_metadata(audio_pd, metadata_path, metadata_cols):\n",
    "  temp_pd = audio_pd[metadata_cols]\n",
    "  \n",
    "  temp_pd.to_json(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:00, 88.50it/s]\n"
     ]
    }
   ],
   "source": [
    "export_audio_data(\n",
    "  hopped_audio_pd, \"waveform\"\n",
    ")\n",
    "\n",
    "# export_audio_metadata(\n",
    "#   og_audio_pd, \n",
    "#   f\"./data/audio/fma_{DATASET_SIZE}_organized_by_label_resampled_rechanneled/metadata.json\",\n",
    "#   og_audio_pd.columns\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
