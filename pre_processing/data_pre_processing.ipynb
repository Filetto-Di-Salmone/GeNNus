{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> GeNNus </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Data pre-processing </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_from_str_to_one_hot(label_str: str): \n",
    "  \n",
    "  if label_str == \"Pop\":\n",
    "    return torch.tensor([1, 0, 0, 0, 0, 0])\n",
    "  \n",
    "  if label_str == \"Hip-Hop\":\n",
    "    return torch.tensor([0, 1, 0, 0, 0, 0])\n",
    "  \n",
    "  if label_str == \"Electronic\":\n",
    "    return torch.tensor([0, 0, 1, 0, 0, 0])\n",
    "  \n",
    "  if label_str == \"Rock\":\n",
    "    return torch.tensor([0, 0, 0, 1, 0, 0])\n",
    "\n",
    "  if label_str == \"Folk\":\n",
    "    return torch.tensor([0, 0, 0, 0, 1, 0])\n",
    "\n",
    "  if label_str == \"Jazz\":\n",
    "    return torch.tensor([0, 0, 0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_audio_data(\n",
    "  path, normalize_audio: bool, audio_num_frames: int\n",
    "):\n",
    "  \n",
    "  audio_tensor_list = []\n",
    "  \n",
    "  num_audio_files_unable_to_open = 0\n",
    "\n",
    "  # TODO use glob instead of this\n",
    "  for path, subdirs, files in tqdm(os.walk(path), colour=\"teal\"):\n",
    "    for name in tqdm(files, colour=\"turquoise\"):\n",
    "        \n",
    "      file_audio_path = os.path.join(path, name)\n",
    "      \n",
    "      try:\n",
    "        waveform, sample_rate = torchaudio.load(\n",
    "          file_audio_path, normalize=normalize_audio,\n",
    "          num_frames=audio_num_frames\n",
    "        )\n",
    "        \n",
    "        label = file_audio_path.split(\"/\")[-2]\n",
    "        label_one_hot = label_from_str_to_one_hot(label)\n",
    "        \n",
    "        audio_tensor_list.append(\n",
    "          {\n",
    "            \"waveform\": waveform.numpy(), \n",
    "            \"og_sample_rate\": sample_rate,\n",
    "            \"label_one_hot\": label_one_hot.numpy(),\n",
    "            \"label\": label,\n",
    "            \"path\": file_audio_path,\n",
    "            \"hop_length\": -1\n",
    "          }\n",
    "        )\n",
    "\n",
    "        \n",
    "      except:\n",
    "        print(f\"[load_audio_data] error while loading {file_audio_path}\")\n",
    "        num_audio_files_unable_to_open += 1\n",
    "        continue\n",
    "  \n",
    "  return pd.DataFrame(audio_tensor_list)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = \"small\"\n",
    "DATASET_NAME = f\"fma_{DATASET_SIZE}_organized_by_label_resampled_rechanneled\"\n",
    "DATASET_FOLDER = \"./data/audio\"\n",
    "# DATASET_FOLDER = \"/mnt/ramdisk\"\n",
    "\n",
    "dataset_path = f\"{DATASET_FOLDER}/{DATASET_NAME}\"\n",
    "\n",
    "AUDIO_NUM_FRAMES = 240000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_audio_data] error while loading ./data/audio/fma_small_organized_by_label_resampled_rechanneled/Pop/023431.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:08<00:00, 112.23it/s]\n",
      "100%|██████████| 999/999 [00:09<00:00, 110.63it/s]\n",
      "100%|██████████| 999/999 [00:08<00:00, 113.68it/s]\n",
      "100%|██████████| 999/999 [00:08<00:00, 111.43it/s]\n",
      "100%|██████████| 999/999 [00:08<00:00, 111.39it/s]\n",
      "100%|██████████| 999/999 [00:09<00:00, 109.48it/s]\n",
      "7it [00:53,  7.69s/it]\n"
     ]
    }
   ],
   "source": [
    "og_audio_pd = load_raw_audio_data(\n",
    "  # path=f\"./data/audio/fma_{DATASET_SIZE}_organized_by_label_resampled_rechanneled/\", \n",
    "  path=dataset_path, \n",
    "  normalize_audio=True, \n",
    "  audio_num_frames=AUDIO_NUM_FRAMES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_audio_pd.iloc[0][\"waveform\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"audio_hop\" --> take one sample ever hop_length elements\n",
    "def resample_audio(og_audio_pd, hop_length: int):\n",
    "  \n",
    "  resampled_audio_tensor_list = []\n",
    "  \n",
    "  for row_index, audio in tqdm(og_audio_pd.iterrows(), colour=\"steelblue\"):\n",
    "    \n",
    "    resampled_waveform = torchaudio.functional.resample(\n",
    "        audio[\"waveform\"], \n",
    "        # as per Torch Audio docs, this is the way of performing \"hopping\" in a \n",
    "        # similar way as the Mel Spectrogram transform does\n",
    "        orig_freq=hop_length, new_freq=1\n",
    "      )\n",
    "    \n",
    "    resampled_audio_tensor_list.append(\n",
    "      {\n",
    "        \"waveform\": resampled_waveform.numpy(), \n",
    "        \"og_sample_rate\": audio[\"og_sample_rate\"],\n",
    "        \"hop_length\": hop_length,\n",
    "        \"label_one_hot\": audio[\"label_one_hot\"].numpy(),\n",
    "        \"label\": audio[\"label\"],\n",
    "        \"path\": audio[\"path\"]\n",
    "      }\n",
    "    )\n",
    "    \n",
    "  return pd.DataFrame(resampled_audio_tensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hop_length = 24\n",
    "\n",
    "# hopped_audio_pd = resample_audio(og_audio_pd, hop_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hopped_audio_pd.iloc[0][\"waveform\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from: https://stackoverflow.com/a/47626762\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "  \n",
    "  def default(self, obj):\n",
    "    \n",
    "    if isinstance(obj, np.ndarray):\n",
    "      return obj.tolist()\n",
    "    \n",
    "    return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def export_audio_data(audio_pd, audio_data_type):\n",
    "  \n",
    "  for row_ind, audio in tqdm(audio_pd.iterrows()):\n",
    "    torchaudio.save(\n",
    "      audio[\"path\"], audio[audio_data_type], audio[\"og_sample_rate\"]\n",
    "  )\n",
    "    \n",
    "def export_mel_spectrogram(audio_pd, path, file_name):\n",
    "  \n",
    "  audio_pd.to_json(f\"{path}/{file_name}\")\n",
    "    \n",
    "    \n",
    "def export_audio_metadata(audio_pd, metadata_path, metadata_cols):\n",
    "  temp_pd = audio_pd[metadata_cols]\n",
    "  \n",
    "  temp_pd.to_json(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_audio_data(\n",
    "#   og_audio_pd, \"waveform\"\n",
    "# )\n",
    "\n",
    "export_audio_metadata(\n",
    "  og_audio_pd, \n",
    "  f\"./data/audio/fma_{DATASET_SIZE}_organized_by_label_resampled_rechanneled/metadata.json\",\n",
    "  og_audio_pd.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mel_spectrogram_transform(\n",
    "  sample_rate: int, n_fft, win_length, hop_length, n_mels\n",
    "):\n",
    "\n",
    "  return torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm='slaney',\n",
    "    onesided=True,\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    "  )\n",
    "\n",
    "def get_mel_spectrogram(\n",
    "  og_audio_pd, n_fft, win_length, hop_length, n_mels\n",
    "):\n",
    "  \n",
    "  mel_spectrogram_tensor_list = []\n",
    "  \n",
    "  for row_ind, audio in og_audio_pd.iterrows():\n",
    "    \n",
    "    mel_spectrogram_transform = get_mel_spectrogram_transform(\n",
    "      audio[\"og_sample_rate\"],\n",
    "      n_fft=n_fft, win_length=win_length, hop_length=hop_length, n_mels=n_mels\n",
    "    )\n",
    "    \n",
    "    mel_spectrogram = mel_spectrogram_transform(audio[\"waveform\"])\n",
    "    \n",
    "    mel_spectrogram_tensor_list.append(\n",
    "      {\n",
    "        \"mel_spectrogram\": mel_spectrogram.numpy(), \n",
    "        \"og_sample_rate\": audio[\"og_sample_rate\"],\n",
    "        \"label_one_hot\": audio[\"label_one_hot\"],\n",
    "        \"label\": audio[\"label\"],\n",
    "        \"mel_spectrogram_config\": {\n",
    "          \"n_fft\": n_fft, \n",
    "          \"win_length\": win_length, \n",
    "          \"hop_length\": hop_length, \n",
    "          \"n_mels\": n_mels\n",
    "        }\n",
    "      }\n",
    "    )\n",
    "    \n",
    "  return pd.DataFrame(mel_spectrogram_tensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft=1024\n",
    "win_length=None\n",
    "hop_length=512\n",
    "n_mels=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectros_pd = get_mel_spectrogram(\n",
    "  og_audio_pd, \n",
    "  n_fft=n_fft, \n",
    "  win_length=win_length, \n",
    "  hop_length=hop_length, \n",
    "  n_mels=n_mels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(mel_spectros_pd.iloc[0][\"mel_spectrogram\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  axs.set_title(title or 'Spectrogram (db)')\n",
    "  axs.set_ylabel(ylabel)\n",
    "  axs.set_xlabel('frame')\n",
    "  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
    "  if xmax:\n",
    "    axs.set_xlim((0, xmax))\n",
    "  fig.colorbar(im, ax=axs)\n",
    "  plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(\n",
    "  mel_spectros_pd.iloc[29][\"mel_spectrogram\"][0], title=\"MelSpectrogram - torchaudio - channel 0\", ylabel='mel freq'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO place all songs of similar genre in the same Tensor, in order to perform the average.\n",
    "# Plot that average and storytell it as the average spectrogram of the genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO place PCA and other non-linear dimensionality reductions here to show the data space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_mel_spectrogram(\n",
    "  mel_spectros_pd, \n",
    "  f\"./data/spectrograms/fma_{DATASET_SIZE}_organized_by_label_resampled_rechanneled\",\n",
    "  \"spectrograms.json\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
