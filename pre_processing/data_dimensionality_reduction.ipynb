{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import gc\n",
    "\n",
    "MANUAL_SEED = 69\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "  \n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "import copy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir_if_absent(dir_path):\n",
    "  \n",
    "  # print(\"making dir: \", dir_path)\n",
    "  \n",
    "  if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_pca(dataset):\n",
    "  inputs = []\n",
    "  outputs = []\n",
    "\n",
    "  dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1024, num_workers=16, \n",
    "  )\n",
    "\n",
    "  iterator_list = list(iter(dataloader))\n",
    "  ### len(iterator_list) --> 23\n",
    "  \n",
    "  # for batch_input, batch_output in tqdm(iter(dataloader), colour=\"#FF00FF\"):\n",
    "  for batch_input, batch_output in tqdm(iterator_list[12:], colour=\"#ff8c00\"):\n",
    "    inputs.append(batch_input) \n",
    "    outputs.append(batch_output)\n",
    "\n",
    "  inputs = torch.cat(tuple(inputs), dim=0).squeeze(1).numpy()\n",
    "  outputs = torch.cat(tuple(outputs), dim=0).numpy()\n",
    "\n",
    "  pca = PCA(n_components=2)\n",
    "\n",
    "  inputs_pca = pca.fit_transform(inputs)\n",
    "\n",
    "  dataset_pca_tuples = (\n",
    "    zip(\n",
    "      inputs_pca[:, 0], inputs_pca[:, 1], outputs, np.argmax(a=outputs, axis=1)\n",
    "    )\n",
    "  )\n",
    "\n",
    "  return pd.DataFrame(\n",
    "    dataset_pca_tuples, \n",
    "    columns = [\n",
    "      \"dataset_pca_1\", \"dataset_pca_2\", \"label_one_hot\", \"label_scalar\"\n",
    "    ]\n",
    "  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = \"extra_large\"\n",
    "DATASET_FOLDER = \"../data/audio\"\n",
    "\n",
    "DATASET_NUM_SAMPLES_PER_SECOND = 8000\n",
    "DATASET_NUM_CHANNELS = 1\n",
    "\n",
    "DATASET_NAME = f\"fma_{DATASET_SIZE}_organized_by_label_resampled_{DATASET_NUM_SAMPLES_PER_SECOND}_rechanneled_{DATASET_NUM_CHANNELS}\"\n",
    "\n",
    "dataset_path = f\"{DATASET_FOLDER}/{DATASET_NAME}\"\n",
    "\n",
    "TRAINING_LOGS_FOLDER = \"./logs\"\n",
    "\n",
    "NORMALIZE_AUDIO = True\n",
    "AUDIO_NUM_FRAMES = 238000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMADataset(Dataset):\n",
    "\n",
    "  def __init__(self, path, normalize_audio, audio_num_frames):\n",
    "    self.path = path\n",
    "    self.normalize_audio = normalize_audio\n",
    "    self.audio_num_frames = audio_num_frames\n",
    "    \n",
    "    self.data = self._load_audio_list()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \n",
    "    try: \n",
    "      \n",
    "      waveform, _ = torchaudio.load(\n",
    "        filepath=self.data[idx], \n",
    "        normalize=self.normalize_audio,\n",
    "        num_frames=self.audio_num_frames\n",
    "      )\n",
    "      \n",
    "      if waveform.shape[1] < self.audio_num_frames:\n",
    "        waveform = self._apply_padding(waveform)\n",
    "      \n",
    "      label = self.data[idx].split(\"/\")[-2]\n",
    "      label_one_hot = self._label_from_str_to_one_hot(label)\n",
    "      \n",
    "      return waveform, label_one_hot\n",
    "    \n",
    "    except Exception as e:\n",
    "      print(f\"Got the following exception for the file {self.data[idx]}\")\n",
    "      print(\"\\n\\n\")\n",
    "      print(e)\n",
    "      \n",
    "  def _apply_padding(self, to_pad):\n",
    "    padding_size = self.audio_num_frames - to_pad.shape[1]\n",
    "    \n",
    "    return torch.nn.functional.pad(\n",
    "      to_pad, (0, padding_size)\n",
    "    )\n",
    "  \n",
    "  def _label_from_str_to_one_hot(self, label_str: str): \n",
    "  \n",
    "    if label_str == \"Pop\":\n",
    "      return torch.tensor([1, 0, 0, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Hip-Hop\":\n",
    "      return torch.tensor([0, 1, 0, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Electronic\":\n",
    "      return torch.tensor([0, 0, 1, 0, 0, 0]).float()\n",
    "    \n",
    "    if label_str == \"Rock\":\n",
    "      return torch.tensor([0, 0, 0, 1, 0, 0]).float()\n",
    "\n",
    "    if label_str == \"Folk\":\n",
    "      return torch.tensor([0, 0, 0, 0, 1, 0]).float()\n",
    "\n",
    "    if label_str == \"Jazz\":\n",
    "      return torch.tensor([0, 0, 0, 0, 0, 1]).float()\n",
    "    \n",
    "  \n",
    "  def _load_audio_list(self):\n",
    "    \n",
    "    audio_path_list = []\n",
    "    \n",
    "    for path, subdirs, files in tqdm(os.walk(self.path), colour=\"magenta\"):\n",
    "      for name in files:\n",
    "          \n",
    "        file_audio_path = os.path.join(path, name)\n",
    "        \n",
    "        audio_path_list.append(file_audio_path)\n",
    "        \n",
    "    return audio_path_list\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ef7adef5974f92acfc327da716c0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fma_dataset = FMADataset(\n",
    "fma_dataset = FMADataset(\n",
    "  path=dataset_path, \n",
    "  normalize_audio=NORMALIZE_AUDIO, \n",
    "  audio_num_frames=AUDIO_NUM_FRAMES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49efaa70c476409c8d988990abbddd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_pca_df = dataset_pca(fma_dataset)\n",
    "\n",
    "make_dir_if_absent(dir_path=f\"{dataset_path}_pca\")\n",
    "\n",
    "dataset_pca_df.to_json(\n",
    "  f\"{dataset_path}_pca/{DATASET_NAME}_pca_2.json\"\n",
    ")\n",
    "\n",
    "# dataset_pca_df.to_csv(\n",
    "#   f\"{dataset_path}_pca/{DATASET_NAME}_pca_1.csv\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
