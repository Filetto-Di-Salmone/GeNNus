{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Attempt_1(nn.Module):\n",
    "  def __init__(self, dropout_p):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.dropout_p = dropout_p\n",
    "    \n",
    "    # First layers, see cell above for full explanation \n",
    "    \n",
    "    self.conv1 = nn.Conv1d(\n",
    "      in_channels=1, out_channels=128, kernel_size=6, stride=4\n",
    "    )\n",
    "    torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "    \n",
    "    # Mid and final layers, see cell above for full explanation\n",
    "    \n",
    "    self.conv3 = nn.Conv1d(\n",
    "      in_channels=128, out_channels=128, kernel_size=3, stride=3\n",
    "    ) \n",
    "    torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
    "    \n",
    "    self.conv4 = nn.Conv1d(\n",
    "      in_channels=128, out_channels=128, kernel_size=3, stride=1\n",
    "    )\n",
    "    torch.nn.init.xavier_uniform_(self.conv4.weight)\n",
    "    self.pool4 = nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "    \n",
    "    self.conv5 = nn.Conv1d(\n",
    "      in_channels=128, out_channels=128, kernel_size=3, stride=1\n",
    "    )\n",
    "    torch.nn.init.xavier_uniform_(self.conv5.weight)\n",
    "    self.pool5 = nn.MaxPool1d(kernel_size=3, stride=3)    \n",
    "    \n",
    "    self.conv6 = nn.Conv1d(\n",
    "      in_channels=128, out_channels=256, kernel_size=3, stride=1\n",
    "    )\n",
    "    torch.nn.init.xavier_uniform_(self.conv6.weight)\n",
    "    self.pool6 = nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "    \n",
    "    self.conv7 = nn.Conv1d(\n",
    "      in_channels=256, out_channels=256, kernel_size=3, stride=1\n",
    "    )\n",
    "    torch.nn.init.xavier_uniform_(self.conv7.weight)\n",
    "    self.pool7 = nn.MaxPool1d(kernel_size=3, stride=3)      \n",
    "    \n",
    "    self.conv8 = nn.Conv1d(\n",
    "      in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "    torch.nn.init.xavier_uniform_(self.conv8.weight)\n",
    "    self.pool8 = nn.MaxPool1d(kernel_size=3,  stride=3)      \n",
    "    \n",
    "    self.conv9 = nn.Conv1d(\n",
    "      in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "    torch.nn.init.xavier_uniform_(self.conv9.weight)\n",
    "    self.pool9 = nn.MaxPool1d(kernel_size=3,  stride=3)      \n",
    "    \n",
    "    self.conv10 = nn.Conv1d(\n",
    "      in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "    torch.nn.init.xavier_uniform_(self.conv10.weight)\n",
    "    self.pool10 = nn.MaxPool1d(kernel_size=3,  stride=3)      \n",
    "    \n",
    "    self.conv11 = nn.Conv1d(\n",
    "      in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "    torch.nn.init.xavier_uniform_(self.conv11.weight)\n",
    "    self.pool11 = nn.MaxPool1d(kernel_size=3,  stride=3)      \n",
    "\n",
    "    self.conv12 = nn.Conv1d(\n",
    "      in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "    torch.nn.init.xavier_uniform_(self.conv12.weight)\n",
    "    self.pool12 = nn.MaxPool1d(kernel_size=3,  stride=3)      \n",
    "\n",
    "    self.conv13 = nn.Conv1d(\n",
    "      in_channels=512, out_channels=512, kernel_size=1, stride=1\n",
    "    )\n",
    "    torch.nn.init.xavier_uniform_(self.conv13.weight)\n",
    "    \n",
    "    # Classification layer\n",
    "    \n",
    "    # Using a conv output layer rather than a fully connected one\n",
    "    self.conv14 = nn.Conv1d(\n",
    "      in_channels=512, out_channels=6, kernel_size=1\n",
    "    )\n",
    "    torch.nn.init.xavier_uniform_(self.conv14.weight)\n",
    "    \n",
    "    self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "    \n",
    "    self.relu = nn.ReLU()\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    self.bn_16 = nn.BatchNorm1d(num_features=16)       \n",
    "    self.bn_32 = nn.BatchNorm1d(num_features=32)       \n",
    "    self.bn_128 = nn.BatchNorm1d(num_features=128)       \n",
    "    self.bn_256 = nn.BatchNorm1d(num_features=256)       \n",
    "    self.bn_512 = nn.BatchNorm1d(num_features=512)       \n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    # First layers, see cell above for full explanation \n",
    "    \n",
    "    x = self.conv1(x)\n",
    "    x = self.bn_128(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    # print(\"-2. x.shape\", x.shape)\n",
    "    \n",
    "    # Mid and final layers, see cell above for full explanation \n",
    "    \n",
    "    x = self.conv3(x)\n",
    "    x = self.bn_128(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    # print(\"3. x.shape\", x.shape)\n",
    "\n",
    "    x = self.conv4(x)\n",
    "    x = self.pool4(x)\n",
    "    x = self.bn_128(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    # print(\"4. x.shape\", x.shape)\n",
    "    \n",
    "    x = self.conv5(x)\n",
    "    x = self.pool5(x)\n",
    "    x = self.bn_128(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    # print(\"5. x.shape\", x.shape)\n",
    "\n",
    "    x = self.conv6(x)\n",
    "    x = self.pool6(x)\n",
    "    x = self.bn_256(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    # print(\"6. x.shape\", x.shape)\n",
    "    \n",
    "    x = self.conv7(x)\n",
    "    x = self.pool7(x)\n",
    "    x = self.bn_256(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    x = self.conv8(x)\n",
    "    # print(\"8_conv. x.shape\", x.shape)\n",
    "    x = self.pool8(x)\n",
    "    # print(\"7_pool. x.shape\", x.shape)\n",
    "    x = self.bn_256(x)\n",
    "    x = self.relu(x)\n",
    "  \n",
    "    x = self.conv9(x)\n",
    "    # print(\"9_conv. x.shape\", x.shape)\n",
    "    x = self.pool9(x)\n",
    "    # print(\"8_pool. x.shape\", x.shape)\n",
    "    x = self.bn_256(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    \n",
    "    x = self.conv10(x)\n",
    "    # print(\"10_conv. x.shape: \", x.shape)\n",
    "    x = self.pool10(x)\n",
    "    # print(\"9_pool. x.shape: \", x.shape)\n",
    "    x = self.bn_256(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    x = self.conv11(x)\n",
    "    # print(\"11_conv. x.shape: \", x.shape)\n",
    "    x = self.pool11(x)\n",
    "    # print(\"11_pool. x.shape: \", x.shape)\n",
    "    x = self.bn_256(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    x = self.conv12(x)\n",
    "    # print(\"12_conv. x.shape: \", x.shape)\n",
    "    x = self.pool12(x)\n",
    "    # print(\"11_pool. x.shape: \", x.shape)\n",
    "    x = self.bn_512(x)\n",
    "    x = self.relu(x)\n",
    "    \n",
    "    x = self.conv13(x)\n",
    "    # print(\"13_conv. x.shape: \", x.shape)\n",
    "    x = self.bn_512(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.dropout(x)\n",
    "    \n",
    "    # Classification layer\n",
    "    x = self.conv14(x)\n",
    "    x = self.sigmoid(x)\n",
    "    \n",
    "    return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
